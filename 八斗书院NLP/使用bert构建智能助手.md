# ä½¿ç”¨bertæ„å»ºæ™ºèƒ½åŠ©æ‰‹

## èƒŒæ™¯

æ™ºèƒ½å¯¹è¯ç³»ç»Ÿï¼ˆå¦‚èŠå¤©æœºå™¨äººã€æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹ï¼‰ã€æœç´¢å¼•æ“å’Œå‚ç›´é¢†åŸŸé—®ç­”ç³»ç»Ÿçš„æ ¸å¿ƒã€‚

ä¸æ˜¯é—²èŠåŠ©æ‰‹ï¼Œç†è§£æŸ¥è¯¢æ„å›¾ï¼Œå¹¶å°†å…¶è½¬åŒ–ä¸ºè®¡ç®—æœºå¯ä»¥ç²¾ç¡®å¤„ç†å’Œæ‰§è¡Œçš„ç»“æ„åŒ–æ•°æ®ã€‚

è¿™é‡Œæ„å»ºå¯¼èˆªåŠ©æ‰‹

## è¾“å…¥å’Œè¾“å‡º

åŸå§‹è¾“å…¥ï¼ˆtextï¼‰: â€œæŸ¥è¯¢è®¸æ˜Œåˆ°ä¸­å±±çš„æ±½è½¦ã€‚â€

è¯­ä¹‰è§£æè¾“å‡ºï¼ˆStructured Dataï¼‰:

```python
intentï¼š æ„å›¾ ï¼ˆç”¨æˆ·æé—®çš„ç±»å‹ï¼‰
QUERYï¼ˆæŸ¥è¯¢ï¼‰ã€BOOKï¼ˆé¢„è®¢ï¼‰ã€CANCELï¼ˆå–æ¶ˆï¼‰ã€COMPAREï¼ˆå¯¹æ¯”ï¼‰
domainï¼š é¢†åŸŸ
slotsï¼šæ§½ä½ï¼ˆå®ä½“ï¼‰
{ â€œDestâ€: â€œä¸­å±±â€, â€œSrcâ€: â€œè®¸æ˜Œâ€ }
```

ç»“æ„åŒ–æŸ¥è¯¢

```sql
SELECT * FROM bus_schedule WHERE src = 'è®¸æ˜Œ' AND dest = 'ä¸­å±±';
```

ç»„ç»‡ä¸ºè‡ªç„¶è¯­è¨€ -ã€‹ è¾“å‡º

## é¡¹ç›®è¿è¡Œæ–¹æ¡ˆ

![2025-09-27_10-03.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/f88f016d-2215-4b43-a90f-9a28878703cc.jpeg)

è¯¥æ–¹æ¡ˆæ˜¯æ—©æœŸçš„ä¸€ä¸ªæ–¹æ¡ˆï¼Œè¿™é‡Œæ§½ä½è¯†åˆ«ä¸æ„å›¾è¯†åˆ«åˆ†åˆ«ä½¿ç”¨ä¸¤ä¸ªbertæ¨¡å‹ã€‚ä½†æ˜¯ä¸¤ä¸ªbertä¸€ä¸ªæ˜¯è®­ç»ƒè¦åˆ†ä¸¤æ¬¡ï¼Œéƒ¨ç½²çš„æ—¶å€™ä¹Ÿä¼šå ç”¨è¾ƒå¤§çš„æ˜¾å­˜ï¼Œäºæ˜¯æ”¹ä¸ºè‡ªå®šä¹‰bertæ¨¡å‹ï¼Œä½¿å…¶æ—¢èƒ½è¯†åˆ«æ„å›¾ä¹Ÿèƒ½è¯†åˆ«æ§½ä½ä¿¡æ¯ã€‚

æœ¬é¡¹ç›®è¦åšçš„èŒƒå›´ä»è¾“å…¥æ–‡æœ¬ä¿¡æ¯å¼€å§‹åˆ°è¾“å‡ºç»“æ„åŒ–JSONä¸ºæ­¢

### æ•°æ®é›†æ„å»º

```python
CITIES = [
    "åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³", "æ­å·", "æˆéƒ½", "æ­¦æ±‰", "è¥¿å®‰", "å—äº¬", "å¤©æ´¥",
    "é‡åº†", "è‹å·", "éƒ‘å·", "é•¿æ²™", "é’å²›", "æ²ˆé˜³", "å¤§è¿", "å®æ³¢", "å¦é—¨", "è®¸æ˜Œ",
    "ä¸­å±±", "ä½›å±±", "ä¸œè", "ç æµ·", "æƒ å·", "æ±Ÿé—¨", "è‚‡åº†", "æ±•å¤´", "æ½®å·", "æ­é˜³"
]

VEHICLES = {
    "æ±½è½¦": "bus",
    "å¤§å·´": "bus",
    "ç­è½¦": "bus",
    "ç«è½¦": "train",
    "é«˜é“": "train",
    "åŠ¨è½¦": "train",
    "é£æœº": "plane",
    "èˆªç­": "plane"
}

DATES = ["ä»Šå¤©", "æ˜å¤©", "åå¤©", "å‘¨ä¸€", "å‘¨äºŒ", "å‘¨ä¸‰", "å‘¨å››", "å‘¨äº”", "å‘¨å…­", "å‘¨æ—¥", "ä¸‹å‘¨ä¸€"]
TIMES = ["ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "æ—©ä¸Š", "ä¸­åˆ", "9ç‚¹", "10ç‚¹", "15ç‚¹", "18ç‚¹"]
SEAT_TYPES = ["ä¸€ç­‰åº§", "äºŒç­‰åº§", "å•†åŠ¡åº§", "ç¡¬åº§", "è½¯åº§", "å§é“º"]
```

```python
# templates.py
TEMPLATES = {
    "QUERY": [
        "æŸ¥ä¸€ä¸‹{src}åˆ°{dest}çš„{vehicle}ã€‚",
        "æˆ‘æƒ³çœ‹çœ‹{src}å»{dest}æœ‰ä»€ä¹ˆ{vehicle}ã€‚",
        "ä»{src}åˆ°{dest}çš„{vehicle}æœ‰å“ªäº›ï¼Ÿ",
        "æœ‰æ²¡æœ‰{src}åˆ°{dest}çš„{vehicle}ï¼Ÿ",
        "æŸ¥è¯¢{date}{src}åˆ°{dest}çš„{vehicle}ã€‚",
        "{src}åˆ°{dest}çš„{vehicle}ä»€ä¹ˆæ—¶å€™å‘è½¦ï¼Ÿ",
        "å¸®æˆ‘æŸ¥{src}åˆ°{dest}çš„{vehicle}ç¥¨ã€‚",
        "çœ‹çœ‹{src}å»{dest}çš„{vehicle}ã€‚",
        "æˆ‘æƒ³çŸ¥é“{src}åˆ°{dest}çš„{vehicle}ä¿¡æ¯ã€‚",
        "æŸ¥{src}åˆ°{dest}çš„{vehicle}ç­æ¬¡ã€‚"
    ],
    "BOOK": [
        "æˆ‘æƒ³è®¢ä¸€å¼ {date}{src}åˆ°{dest}çš„{vehicle}ç¥¨ã€‚",
        "å¸®æˆ‘é¢„è®¢{date}{time}{src}åˆ°{dest}çš„{vehicle}ã€‚",
        "è®¢ç¥¨ï¼š{src}åˆ°{dest}ï¼Œ{date}ï¼Œ{vehicle}ã€‚",
        "æˆ‘è¦ä¹°{date}ä»{src}åˆ°{dest}çš„{vehicle}ç¥¨ï¼Œ{passenger_count}ä¸ªäººã€‚",
        "é¢„è®¢{src}åˆ°{dest}çš„{vehicle}ï¼Œ{date}å‡ºå‘ï¼Œ{seat_type}ã€‚",
        "å¸®æˆ‘è®¢{date}{src}å»{dest}çš„{vehicle}ï¼Œ{passenger_count}å¼ ã€‚",
        "æˆ‘æƒ³è®¢{date}{time}ä»{src}åˆ°{dest}çš„{vehicle}ï¼Œ{seat_type}ã€‚",
        "è®¢ä¸€å¼ {src}åˆ°{dest}çš„{vehicle}ç¥¨ï¼Œ{date}ï¼Œ{passenger_count}äººã€‚",
        "æˆ‘è¦é¢„è®¢{date}ä»{src}åˆ°{dest}çš„{vehicle}ï¼Œ{seat_type}ã€‚",
        "å¸®æˆ‘è®¢{src}åˆ°{dest}çš„{vehicle}ï¼Œ{date}ï¼Œ{passenger_count}ä¸ªäººã€‚"
    ],
    "COMPARE": [
        "æ¯”è¾ƒä¸€ä¸‹{src}åˆ°{dest}å{vehicle1}å’Œ{vehicle2}å“ªä¸ªå¿«ï¼Ÿ",
        "{src}åˆ°{dest}ï¼Œ{vehicle1}å’Œ{vehicle2}çš„ä»·æ ¼å¯¹æ¯”ã€‚",
        "å¸®æˆ‘å¯¹æ¯”{src}åˆ°{dest}çš„{vehicle1}å’Œ{vehicle2}ã€‚",
        "{src}å»{dest}ï¼Œ{vehicle1}è·Ÿ{vehicle2}å“ªä¸ªä¾¿å®œï¼Ÿ",
        "æ¯”è¾ƒ{src}åˆ°{dest}çš„{vehicle1}ã€{vehicle2}å’Œ{vehicle3}çš„æ—¶é—´ã€‚",
        "æˆ‘æƒ³çŸ¥é“{src}åˆ°{dest}å{vehicle1}å’Œ{vehicle2}æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚",
        "å¯¹æ¯”ä¸€ä¸‹{src}åˆ°{dest}çš„{vehicle1}å’Œ{vehicle2}çš„ç¥¨ä»·ã€‚",
        "{src}åˆ°{dest}ï¼Œ{vehicle1}å’Œ{vehicle2}å“ªä¸ªæ›´æ–¹ä¾¿ï¼Ÿ",
        "å¸®æˆ‘çœ‹çœ‹{src}åˆ°{dest}çš„{vehicle1}å’Œ{vehicle2}æ€ä¹ˆé€‰ã€‚",
        "æ¯”è¾ƒ{date}{src}åˆ°{dest}çš„{vehicle1}å’Œ{vehicle2}ã€‚"
    ],
    "CANCEL": [
        "å–æ¶ˆè®¢å•{booking_id}ã€‚",
        "æˆ‘æƒ³å–æ¶ˆ{src}åˆ°{dest}çš„è®¢å•ï¼Œ{date}çš„ã€‚",
        "å¸®æˆ‘å–æ¶ˆé¢„è®¢ï¼š{src}åˆ°{dest}ï¼Œ{date}ã€‚",
        "å–æ¶ˆæˆ‘çš„{vehicle}ç¥¨ï¼Œ{src}åˆ°{dest}ï¼Œ{date}ã€‚",
        "æˆ‘è¦é€€æ‰{date}ä»{src}åˆ°{dest}çš„{vehicle}ç¥¨ã€‚",
        "å–æ¶ˆè®¢å•ï¼Œ{src}åˆ°{dest}ï¼Œ{date}ã€‚",
        "è¯·å¸®æˆ‘å–æ¶ˆ{booking_id}è¿™ä¸ªè®¢å•ã€‚",
        "æˆ‘æƒ³é€€è®¢{src}åˆ°{dest}çš„{vehicle}ï¼Œ{date}ã€‚",
        "å–æ¶ˆæˆ‘çš„è¡Œç¨‹ï¼š{src}åˆ°{dest}ï¼Œ{date}ã€‚",
        "é€€æ‰{date}{src}åˆ°{dest}çš„{vehicle}ç¥¨ã€‚"
    ]
}
```

è¿™é‡Œåˆ©ç”¨ä¸Šè¿°ä¸¤ä¸ªäººå·¥æ„å»ºçš„æ•°æ®æ¥éšæœºç»„åˆç”Ÿæˆæ•°æ®é›†

```python
# generate_data.py
import random
import json
from itertools import product
from entities import CITIES, VEHICLES, DATES, TIMES, SEAT_TYPES

# åŠ è½½æ¨¡æ¿
from templates import TEMPLATES


def get_random_booking_id():
    return f"BK{random.randint(100000, 999999)}"


def bio_tag(text, entities):
    """ä¸ºæ–‡æœ¬ç”Ÿæˆ BIO æ ‡ç­¾"""
    tokens = list(text)
    labels = ["O"] * len(tokens)

    for slot_name, value in entities.items():
        if not value:
            continue
        # å¤„ç†å¤šå€¼æ§½ä½ï¼ˆå¦‚ VehicleTypesï¼‰
        values = value if isinstance(value, list) else [value]
        for v in values:
            start = text.find(v)
            while start != -1:
                labels[start] = f"B-{slot_name}"
                for i in range(1, len(v)):
                    if start + i < len(labels):
                        labels[start + i] = f"I-{slot_name}"
                start = text.find(v, start + 1)
    return labels


def generate_samples():
    samples = []
    intent_counts = {"QUERY": 600, "BOOK": 600, "COMPARE": 400, "CANCEL": 400}

    for intent, count in intent_counts.items():
        for _ in range(count):
            # éšæœºé€‰å®ä½“
            src = random.choice(CITIES)
            dest = random.choice([c for c in CITIES if c != src])
            vehicle = random.choice(list(VEHICLES.keys()))
            date = random.choice(DATES)
            time = random.choice(TIMES)
            passenger = random.randint(1, 5)
            seat = random.choice(SEAT_TYPES)
            booking_id = get_random_booking_id()

            # ä¸º COMPARE é€‰å¤šä¸ªäº¤é€šå·¥å…·
            if intent == "COMPARE":
                vehicles = random.sample(list(VEHICLES.keys()), k=min(2, len(VEHICLES)))
                template = random.choice(TEMPLATES[intent])
                # æ›¿æ¢ {vehicle1}, {vehicle2}
                text = template.format(
                    src=src, dest=dest, date=date,
                    vehicle1=vehicles[0],
                    vehicle2=vehicles[1] if len(vehicles) > 1 else vehicles[0],
                    vehicle3=random.choice(list(VEHICLES.keys()))
                )
                slots = {
                    "Src": src,
                    "Dest": dest,
                    "Date": date if "date" in template else None,
                    "VehicleTypes": vehicles
                }
            elif intent == "CANCEL":
                text = random.choice(TEMPLATES[intent]).format(
                    src=src, dest=dest, date=date, vehicle=vehicle,booking_id=booking_id
                )
                slots = {"Src": src, "Dest": dest, "Date": date,"booking_id":booking_id}
            else:
                template = random.choice(TEMPLATES[intent])
                text = template.format(
                    src=src, dest=dest, vehicle=vehicle,
                    date=date, time=time,
                    passenger_count=passenger, seat_type=seat
                )
                slots = {
                    "Src": src,
                    "Dest": dest,
                    "VehicleType": vehicle,
                    "Date": date if "date" in template.lower() else None,
                    "Time": time if "time" in template.lower() else None,
                }
                if intent == "BOOK":
                    slots.update({
                        "PassengerCount": str(passenger),
                        "SeatType": seat
                    })

            # æ¸…ç† None å€¼
            slots = {k: v for k, v in slots.items() if v is not None}

            # ç”Ÿæˆ BIO æ ‡ç­¾
            bio_labels = bio_tag(text, slots)

            samples.append({
                "text": text,
                "intent": intent,
                "slots_bio": bio_labels
            })

    return samples


if __name__ == "__main__":
    samples = generate_samples()
    print(f"ç”Ÿæˆ {len(samples)} æ¡æ ·æœ¬")

    # ä¿å­˜ä¸º JSONL
    with open("train_data.jsonl", "w", encoding="utf-8") as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + "\n")

    # æ‰“å°ç¤ºä¾‹
    print("\nç¤ºä¾‹ï¼š")
    for i in range(3):
        print(samples[i])
```

ç”Ÿæˆçš„æ•°æ®é›†å¦‚ä¸‹ï¼š

```json
{"text": "æ­¦æ±‰åˆ°è‚‡åº†çš„æ±½è½¦ä»€ä¹ˆæ—¶å€™å‘è½¦ï¼Ÿ", "intent": "QUERY", "slots_bio": ["B-Src", "I-Src", "O", "B-Dest", "I-Dest", "O", "B-VehicleType", "I-VehicleType", "O", "O", "O", "O", "O", "O", "O"]}
{"text": "ä»è‚‡åº†åˆ°ä¸Šæµ·çš„é«˜é“æœ‰å“ªäº›ï¼Ÿ", "intent": "QUERY", "slots_bio": ["O", "B-Src", "I-Src", "O", "B-Dest", "I-Dest", "O", "B-VehicleType", "I-VehicleType", "O", "O", "O", "O"]}
{"text": "æŸ¥å®æ³¢åˆ°å¤§è¿çš„åŠ¨è½¦ç­æ¬¡ã€‚", "intent": "QUERY", "slots_bio": ["O", "B-Src", "I-Src", "O", "B-Dest", "I-Dest", "O", "B-VehicleType", "I-VehicleType", "O", "O", "O"]}

```

### bertæ¨¡å‹æ„å»º

è¿™é‡Œä¸»è¦æ˜¯æ¨¡ä»¿å®˜æ–¹huggingfaceä¸­bertä¸åŒä¸‹æ¸¸ä»»åŠ¡æ¨¡å‹çš„å®ç°

```python
from dataclasses import dataclass
from typing import Optional,Union,Tuple,Dict
import torch
import torch.nn as nn
from transformers.modeling_outputs import ModelOutput
from transformers import (
    BertPreTrainedModel,
    BertModel
)
@dataclass
class SequenceAndTokenClassifierOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    intent_logits: Optional[torch.FloatTensor] = None
    slot_logits: Optional[torch.FloatTensor] = None
    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None
    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None


class Bert4TextAndTokenClassification(BertPreTrainedModel):
    def __init__(self, config, seq_num_labels, token_num_labels):
        super().__init__(config)
        self.seq_num_labels = seq_num_labels
        self.token_num_labels = token_num_labels
        self.config = config

        self.bert = BertModel(config)
        self.sequence_classification = nn.Sequential(
            nn.Dropout(config.hidden_dropout_prob),
            nn.Linear(config.hidden_size, seq_num_labels),
        )
        self.token_classification = nn.Sequential(
            nn.Dropout(config.hidden_dropout_prob),
            nn.Linear(config.hidden_size, token_num_labels),
        )
        self.post_init()

    def forward(
            self,
            input_ids: Optional[torch.Tensor] = None,
            attention_mask: Optional[torch.Tensor] = None,
            token_type_ids: Optional[torch.Tensor] = None,
            position_ids: Optional[torch.Tensor] = None,
            head_mask: Optional[torch.Tensor] = None,
            inputs_embeds: Optional[torch.Tensor] = None,
            intent_labels: Optional[torch.Tensor] = None,
            slot_labels: Optional[torch.Tensor] = None,
            output_attentions: Optional[bool] = None,
            output_hidden_states: Optional[bool] = None,
            return_dict: Optional[bool] = None,
    ) -> Union[Tuple[torch.Tensor], SequenceAndTokenClassifierOutput]:
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        pooled_output = outputs[1]  # [CLS]
        sequence_output = outputs[0]  # [batch, seq_len, hidden]

        intent_logits = self.sequence_classification(pooled_output)
        slot_logits = self.token_classification(sequence_output)

        loss = None
        if intent_labels is not None and slot_labels is not None:
            # æ„å›¾ï¼šå•æ ‡ç­¾åˆ†ç±» â†’ CrossEntropyLoss
            intent_loss = nn.CrossEntropyLoss()(intent_logits, intent_labels)
            # æ§½ä½ï¼šåºåˆ—æ ‡æ³¨ â†’ CrossEntropyLoss (ignore_index=-100)
            slot_loss = nn.CrossEntropyLoss(ignore_index=-100)(
                slot_logits.view(-1, self.token_num_labels), slot_labels.view(-1)
            )
            loss = intent_loss + 10*slot_loss

        if not return_dict:
            output = (intent_logits, slot_logits) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return SequenceAndTokenClassifierOutput(
            loss=loss,
            intent_logits=intent_logits,
            slot_logits=slot_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```


### åŠ è½½æ•°æ®é›†

```python
intents = ['QUERY', 'BOOK', 'COMPARE', 'CANCEL']
intents2id = {intent: id for id, intent in enumerate(intents)}
id2intents = {id: intent for intent, id in intents2id.items()}
slots = ['O','B-Time','I-Time', 'B-SeatType','I-SeatType', 'B-VehicleTypes','I-VehicleTypes', 'B-Dest','I-Dest',
         'B-booking_id','I-booking_id', 'B-VehicleType','I-VehicleType',
         'B-Date','I-Date', 'B-PassengerCount', 'B-Src','I-Src',
            ]
slots2id = {slot: id for id, slot in enumerate(slots)}
id2slots = {id: slot for slot, id in slots2id.items()}
tokenizer = BertTokenizerFast.from_pretrained("../../bert-base-chinese")
def load_data(
    file_path: str = "train_data.jsonl",
    test_size: float = 0.2,
    random_state: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œåˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶è¿›è¡Œæ•°æ®æ ¡éªŒã€‚

    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        random_state: éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°

    Returns:
        (train_df, test_df): åˆ’åˆ†åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›† DataFrame

    Raises:
        AssertionError: å¦‚æœæ•°æ®ä¸­çš„æ„å›¾æˆ–æ§½ä½æ ‡ç­¾ä¸é¢„å®šä¹‰ä¸ä¸€è‡´
        FileNotFoundError: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
        json.JSONDecodeError: å¦‚æœ JSON æ ¼å¼é”™è¯¯
    """
    # 1. åŠ è½½æ•°æ®
    samples = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue  # è·³è¿‡ç©ºè¡Œ
                try:
                    sample = json.loads(line)
                    # å¯é€‰ï¼šæ ¡éªŒå¿…è¦å­—æ®µ
                    if not all(k in sample for k in ["text", "intent", "slots_bio"]):
                        raise ValueError(f"Missing keys in line {line_num}")
                    samples.append(sample)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Invalid JSON at line {line_num}: {e}")
    except FileNotFoundError:
        raise FileNotFoundError(f"Data file not found: {file_path}")

    if not samples:
        raise ValueError("No valid samples loaded from the file.")

    # 2. è½¬ä¸º DataFrame å¹¶æ ¡éªŒæ ‡ç­¾
    df = pd.DataFrame(samples)

    # æ ¡éªŒæ„å›¾æ ‡ç­¾
    data_intents: Set[str] = set(df["intent"].unique())
    expected_intents: Set[str] = set(intents)
    if data_intents != expected_intents:
        missing = expected_intents - data_intents
        extra = data_intents - expected_intents
        msg = []
        if missing:
            msg.append(f"Missing intents in data: {missing}")
        if extra:
            msg.append(f"Unexpected intents in data: {extra}")
        raise AssertionError("Intent label mismatch!\n" + "\n".join(msg))

    # æ ¡éªŒæ§½ä½æ ‡ç­¾
    all_slot_tags: List[str] = [tag for tags in df["slots_bio"] for tag in tags]
    data_slots: Set[str] = set(all_slot_tags)
    expected_slots: Set[str] = set(slots)
    if data_slots != expected_slots:
        missing = expected_slots - data_slots
        extra = data_slots - expected_slots
        msg = []
        if missing:
            msg.append(f"Missing slot tags in data: {missing}")
        if extra:
            msg.append(f"Unexpected slot tags in data: {extra}")
        raise AssertionError("Slot label mismatch!\n" + "\n".join(msg))

    # 3. åˆ’åˆ†æ•°æ®é›†
    train_df, test_df = train_test_split(
        df,
        test_size=test_size,
        random_state=random_state,
        stratify=df["intent"]  # æŒ‰æ„å›¾åˆ†å±‚æŠ½æ ·ï¼Œä¿è¯åˆ†å¸ƒä¸€è‡´
    )

    print(f"Total samples: {len(df)}")
    print(f"Train samples: {len(train_df)}, Test samples: {len(test_df)}")

    return train_df, test_df
```

è¿™é‡Œæ˜¯ä»jsonæ–‡ä»¶ä¸­è¯»å–å¹¶è½¬ä¸ºäº†pandasçš„æ ¼å¼ï¼Œä¸€ä¸ªæ˜¯pandasåœ¨pycharmä¸­è°ƒè¯•è§‚å¯Ÿå¾ˆæ–¹ä¾¿ï¼Œåœ¨å†ä¸€ä¸ªæ˜¯pandasè½¬å…¶ä»–æ ¼å¼ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚


### pandasæ•°æ®é›†--> huggingfaceæ•°æ®é›†

```python
    train_dataset,test_dataset = load_data()

    model = Bert4TextAndTokenClassification.from_pretrained("../../bert-base-chinese", seq_num_labels=len(intents),
                                              token_num_labels=len(slots))

    train_dataset = Dataset.from_pandas(train_dataset)
    train_dataset = train_dataset.map(
        tokenize_and_align_labels,
        batched=True,
        remove_columns=train_dataset.column_names  # åˆ é™¤åŸå§‹åˆ—
    )

    test_dataset = Dataset.from_pandas(test_dataset)
    test_dataset = test_dataset.map(
        tokenize_and_align_labels,
        batched=True,
        remove_columns=test_dataset.column_names
    )
```

è¿™é‡Œè¿ç”¨åˆ°äº†å¯¹é½å‡½æ•°ï¼Œä¸»è¦æ˜¯æ§½ä½è¯†åˆ«è¿™é‡Œéœ€è¦å°†tokenå’Œlabelè¿›è¡Œå¯¹é½

```python
def tokenize_and_align_labels(examples):
    # å‘Šè¯‰ tokenizer è¾“å…¥å·²æ˜¯å­—ç¬¦åˆ—è¡¨
    tokenized_inputs = tokenizer(
        [list(text) for text in examples["text"]],  # æŒ‰å­—åˆ‡åˆ†
        is_split_into_words=True,
        padding=True,
        truncation=True,
        max_length=128
    )

    intent_labels = [intents2id[intent] for intent in examples["intent"]]

    slot_labels = []
    for i, label in enumerate(examples["slots_bio"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            if word_idx is None:
                # [CLS], [SEP], padding â†’ -100
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                # é¦–æ¬¡å‡ºç°çš„ token â†’ ä½¿ç”¨åŸå§‹æ ‡ç­¾
                label_ids.append(slots2id[label[word_idx]])
            else:
                # subword â†’ -100ï¼ˆå¿½ç•¥ï¼‰
                label_ids.append(-100)
            previous_word_idx = word_idx
        slot_labels.append(label_ids)

    return {
        "input_ids": tokenized_inputs["input_ids"],
        "attention_mask": tokenized_inputs["attention_mask"],
        "intent_labels": intent_labels,
        "slot_labels": slot_labels,
    }
```

è¿™é‡Œæˆ‘ä»¬å·²ç»æˆåŠŸå°†æ•°æ®é›†è½¬ä¸ºhuggingfaceä¸­trainerå¯ä»¥ç”¨æ¥è®­ç»ƒçš„æ ¼å¼

### è®¾ç½®è®­ç»ƒå‚æ•°

```python
    training_args = TrainingArguments(
        output_dir='./Results',
        num_train_epochs=8,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        warmup_steps=500,
        learning_rate=2e-5,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=20,
        eval_strategy="epoch",  # ä¿®æ­£
        save_strategy="epoch",
        save_total_limit=2,
        load_best_model_at_end=True,
        metric_for_best_model="f1_macro",
        greater_is_better=True,
        report_to="wandb",
        run_name="my-Results-run",
        logging_strategy="steps",
        remove_unused_columns=False,  # ğŸ‘ˆ å…³é”®ï¼ä¿ç•™è‡ªå®šä¹‰åˆ—
    )


    # å®ä¾‹åŒ– Trainer
    trainer = Trainer(
        model=model,                         # è¦è®­ç»ƒçš„æ¨¡å‹
        args=training_args,                  # è®­ç»ƒå‚æ•°
        train_dataset=train_dataset,         # è®­ç»ƒæ•°æ®é›†
        eval_dataset=test_dataset,           # è¯„ä¼°æ•°æ®é›†
        compute_metrics=compute_metrics,     # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
    )

    # å¼€å§‹è®­ç»ƒæ¨¡å‹
    trainer.train()
    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
    trainer.evaluate()
    trainer.save_model("best")
    print("Done")
```

è¿™é‡Œä¸»è¦æ˜¯éœ€è¦æ³¨æ„`training_args`çš„æœ€åä¸€ä¸ªå‚æ•°ï¼Œå¿…é¡»è®¾ç½®ä¸ºFalseï¼Œä¸ç„¶huggingfaceä¼šé»˜è®¤åˆ é™¤æ‰ä¸è®¤è¯†çš„åˆ—ã€‚

ç„¶åè¿™é‡Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰è¯„ä¼°å‡½æ•°

```python

```
