# ‰ΩøÁî®bertËøõË°åÊñáÊú¨ÂàÜÁ±ª

## Â§ñÂçñËØÑËÆ∫Êï∞ÊçÆÈõÜ

```
label,review
1,ÂæàÂø´ÔºåÂ•ΩÂêÉÔºåÂë≥ÈÅìË∂≥ÔºåÈáèÂ§ß
1,Ê≤°ÊúâÈÄÅÊ∞¥Ê≤°ÊúâÈÄÅÊ∞¥Ê≤°ÊúâÈÄÅÊ∞¥
1,ÈùûÂ∏∏Âø´ÔºåÊÄÅÂ∫¶Â•Ω„ÄÇ
1,Êñπ‰æøÔºåÂø´Êç∑ÔºåÂë≥ÈÅìÂèØÂè£ÔºåÂø´ÈÄíÁªôÂäõ
1,ËèúÂë≥ÈÅìÂæàÊ£íÔºÅÈÄÅÈ§êÂæàÂèäÊó∂ÔºÅ
1,‰ªäÂ§©Â∏àÂÇÖÊòØ‰∏çÊòØÊâãÊäñ‰∫ÜÔºåÂæÆËæ£Ê†ºÂ§ñËæ£ÔºÅ
1,"ÈÄÅÈ§êÂø´,ÊÄÅÂ∫¶‰πüÁâπÂà´Â•Ω,ËæõËã¶Âï¶Ë∞¢Ë∞¢"
```

## bertÂæÆË∞É

```python
# ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
```

```python
# Âä†ËΩΩÊï∞ÊçÆÈõÜ
dataset_path = "waimai_10k.csv"

dataset_df = pd.read_csv(dataset_path, sep=",", header=None)[1:]

print(dataset_df.shape)
print(dataset_df.head())
```

```python
# ÂàùÂßãÂåñ LabelEncoderÔºåÁî®‰∫éÂ∞ÜÊñáÊú¨Ê†áÁ≠æËΩ¨Êç¢‰∏∫Êï∞Â≠óÊ†áÁ≠æ
lbl = LabelEncoder()
# ÊãüÂêàÊï∞ÊçÆÂπ∂ËΩ¨Êç¢Ââç500‰∏™Ê†áÁ≠æÔºåÂæóÂà∞Êï∞Â≠óÊ†áÁ≠æ
labels = lbl.fit_transform(dataset_df[0].values[:])
texts = list(dataset_df[1].values[:])

print(len(texts))

# ÂàÜÂâ≤Êï∞ÊçÆ‰∏∫ËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜ
x_train, x_test, train_labels, test_labels = train_test_split(
    texts,             # ÊñáÊú¨Êï∞ÊçÆ
    labels,            # ÂØπÂ∫îÁöÑÊï∞Â≠óÊ†áÁ≠æ
    test_size=0.2,     # ÊµãËØïÈõÜÊØî‰æã‰∏∫20%
    stratify=labels    # Á°Æ‰øùËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÁöÑÊ†áÁ≠æÂàÜÂ∏É‰∏ÄËá¥
)
```

```python
# ‰ªéÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂä†ËΩΩÂàÜËØçÂô®ÂíåÊ®°Âûã
tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('./bert-base-chinese', num_labels=2)
```

```python
# ‰ΩøÁî®ÂàÜËØçÂô®ÂØπËÆ≠ÁªÉÈõÜÂíåÊµãËØïÈõÜÁöÑÊñáÊú¨ËøõË°åÁºñÁ†Å
# truncation=TrueÔºöÂ¶ÇÊûúÊñáÊú¨ËøáÈïøÂàôÊà™Êñ≠
# padding=TrueÔºöÂØπÈΩêÊâÄÊúâÂ∫èÂàóÈïøÂ∫¶ÔºåÂ°´ÂÖÖÂà∞ÊúÄÈïø
# max_length=64ÔºöÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=64)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=64)
```

```python
# Â∞ÜÁºñÁ†ÅÂêéÁöÑÊï∞ÊçÆÂíåÊ†áÁ≠æËΩ¨Êç¢‰∏∫ Hugging Face `datasets` Â∫ìÁöÑ Dataset ÂØπË±°
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],           # ÊñáÊú¨ÁöÑtoken ID
    'attention_mask': train_encodings['attention_mask'], # Ê≥®ÊÑèÂäõÊé©Á†Å
    'labels': train_labels                               # ÂØπÂ∫îÁöÑÊ†áÁ≠æ
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels
})

print(train_dataset.shape)
```

```python
# ÂÆö‰πâÁî®‰∫éËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†áÁöÑÂáΩÊï∞
def compute_metrics(eval_pred):
    # eval_pred ÊòØ‰∏Ä‰∏™ÂÖÉÁªÑÔºåÂåÖÂê´Ê®°ÂûãÈ¢ÑÊµãÁöÑ logits ÂíåÁúüÂÆûÁöÑÊ†áÁ≠æ
    logits, labels = eval_pred
    # ÊâæÂà∞ logits ‰∏≠ÊúÄÂ§ßÂÄºÁöÑÁ¥¢ÂºïÔºåÂç≥È¢ÑÊµãÁöÑÁ±ªÂà´
    predictions = np.argmax(logits, axis=-1)
    # ËÆ°ÁÆóÈ¢ÑÊµãÂáÜÁ°ÆÁéáÂπ∂ËøîÂõû‰∏Ä‰∏™Â≠óÂÖ∏
    return {'accuracy': (predictions == labels).mean()}
```

```python
# ÈÖçÁΩÆËÆ≠ÁªÉÂèÇÊï∞
training_args = TrainingArguments(
    output_dir='./results',              # ËÆ≠ÁªÉËæìÂá∫ÁõÆÂΩïÔºåÁî®‰∫é‰øùÂ≠òÊ®°ÂûãÂíåÁä∂ÊÄÅ
    num_train_epochs=8,                  # ËÆ≠ÁªÉÁöÑÊÄªËΩÆÊï∞
    per_device_train_batch_size=32,      # ËÆ≠ÁªÉÊó∂ÊØè‰∏™ËÆæÂ§áÔºàGPU/CPUÔºâÁöÑÊâπÊ¨°Â§ßÂ∞è
    per_device_eval_batch_size=32,       # ËØÑ‰º∞Êó∂ÊØè‰∏™ËÆæÂ§áÁöÑÊâπÊ¨°Â§ßÂ∞è
    warmup_steps=500,                    # Â≠¶‰π†ÁéáÈ¢ÑÁÉ≠ÁöÑÊ≠•Êï∞ÔºåÊúâÂä©‰∫éÁ®≥ÂÆöËÆ≠ÁªÉ
    weight_decay=0.01,                   # ÊùÉÈáçË°∞ÂáèÔºåÁî®‰∫éÈò≤Ê≠¢ËøáÊãüÂêà
    logging_dir='./logs',                # Êó•ÂøóÂ≠òÂÇ®ÁõÆÂΩï
    logging_steps=100,                   # ÊØèÈöî100Ê≠•ËÆ∞ÂΩï‰∏ÄÊ¨°Êó•Âøó
    eval_strategy="epoch",               # ÊØèËÆ≠ÁªÉÂÆå‰∏Ä‰∏™ epoch ËøõË°å‰∏ÄÊ¨°ËØÑ‰º∞
    save_strategy="best",               # ÊØèËÆ≠ÁªÉÂÆå‰∏Ä‰∏™ epoch ‰øùÂ≠ò‰∏ÄÊ¨°Ê®°Âûã
    load_best_model_at_end=True,         # ËÆ≠ÁªÉÁªìÊùüÂêéÂä†ËΩΩÊïàÊûúÊúÄÂ•ΩÁöÑÊ®°Âûã
)

# ÂÆû‰æãÂåñ Trainer
trainer = Trainer(
    model=model,                         # Ë¶ÅËÆ≠ÁªÉÁöÑÊ®°Âûã
    args=training_args,                  # ËÆ≠ÁªÉÂèÇÊï∞
    train_dataset=train_dataset,         # ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ
    eval_dataset=test_dataset,           # ËØÑ‰º∞Êï∞ÊçÆÈõÜ
    compute_metrics=compute_metrics,     # Áî®‰∫éËÆ°ÁÆóËØÑ‰º∞ÊåáÊ†áÁöÑÂáΩÊï∞
)

# ÂºÄÂßãËÆ≠ÁªÉÊ®°Âûã
trainer.train()
# Âú®ÊµãËØïÈõÜ‰∏äËøõË°åÊúÄÁªàËØÑ‰º∞
trainer.evaluate()
trainer.save_model("best")
print("Done")
```

## fastapiÈÉ®ÁΩ≤

### Êï∞ÊçÆÊé•Âè£ÂÆö‰πâ

```python
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Union, Optional
```

BaseModel: pydantic ÁöÑÊ†∏ÂøÉÁ±ªÔºåÊâÄÊúâÊï∞ÊçÆÊ®°ÂûãÈÉΩÁªßÊâøËá™ÂÆÉÔºåÁî®‰∫éÂÆö‰πâÁªìÊûÑÂåñÊï∞ÊçÆ„ÄÇ
Field: Áî®‰∫éÂØπÂ≠óÊÆµÊ∑ªÂä†È¢ùÂ§ñ‰ø°ÊÅØÔºåÂ¶ÇÈªòËÆ§ÂÄº„ÄÅÊèèËø∞„ÄÅÁ∫¶ÊùüÁ≠â„ÄÇ
Dict, List, Any, Union, Optional: Êù•Ëá™ typing Ê®°ÂùóÔºåÁî®‰∫éÁ±ªÂûãÊ≥®Ëß£Ôºö

* Optional[str] Á≠â‰ª∑‰∫é Union[str, None]ÔºåË°®Á§∫ËØ•Â≠óÊÆµÂèØ‰∏∫Á©∫ÔºàÂç≥ÂèØ‰ª•ÊòØ str Êàñ NoneÔºâ„ÄÇ
  Union[A, B] Ë°®Á§∫Â≠óÊÆµÂèØ‰ª•ÊòØ A Á±ªÂûãÊàñ B Á±ªÂûã„ÄÇ
  List[str] Ë°®Á§∫Â≠óÁ¨¶‰∏≤ÂàóË°®„ÄÇ
  Any Ë°®Á§∫‰ªªÊÑèÁ±ªÂûãÔºà‰∏çÊé®ËçêËøáÂ∫¶‰ΩøÁî®Ôºå‰ºöÂ§±ÂéªÁ±ªÂûãÂÆâÂÖ®Ôºâ„ÄÇ

```python
# ËØ∑Ê±ÇÊ®°Âûã
class TextClassifyRequest(BaseModel):
    """
    ËØ∑Ê±ÇÊ†ºÂºè
    """
    request_id: Optional[str] = Field(..., description="ËØ∑Ê±Çid, Êñπ‰æøË∞ÉËØï")
    request_text: Union[str, List[str]] = Field(..., description="ËØ∑Ê±ÇÊñáÊú¨„ÄÅÂ≠óÁ¨¶‰∏≤ÊàñÂàóË°®")
```

`request_id: Optional[str] = Field(..., description="ËØ∑Ê±Çid, Êñπ‰æøË∞ÉËØï")`

* Á±ªÂûãÔºö`Optional[str]` ‚Üí ÂèØ‰ª•ÊòØÂ≠óÁ¨¶‰∏≤Ôºå‰πüÂèØ‰ª•ÊòØ `None`ÔºàÂç≥Ëøô‰∏™Â≠óÊÆµ‰∏çÊòØÂøÖÈ°ª‰º†ÁöÑÔºâ„ÄÇ
* `Field(..., ...)`Ôºö
  * Á¨¨‰∏Ä‰∏™ `...` Ë°®Á§∫Ëøô‰∏™Â≠óÊÆµÊòØ**ÂøÖÂ°´È°π**ÔºàÂç≥‰ΩøÁ±ªÂûãÊòØ `Optional`Ôºå‰ΩÜÂ¶ÇÊûú‰∏ç‰º†ÂÄºÔºå‰πü‰ºöÊä•ÈîôÔºâ„ÄÇ
  * `description`ÔºöÂ≠óÊÆµÊèèËø∞Ôºå‰ºöÂú® API ÊñáÊ°£ÔºàÂ¶Ç SwaggerÔºâ‰∏≠ÊòæÁ§∫„ÄÇ
* Áî®ÈÄîÔºöÂÆ¢Êà∑Á´Ø‰º†‰∏Ä‰∏™ËØ∑Ê±Ç IDÔºå‰æø‰∫éÊúçÂä°Á´ØÊó•ÂøóËøΩË∏™ÂíåË∞ÉËØï„ÄÇ

> ‚ö†Ô∏è Ê≥®ÊÑèÔºö`Optional[str]` + `...` ÊÑèÂë≥ÁùÄÔºö**ÂèØ‰ª•‰º† nullÔºå‰ΩÜ‰∏çËÉΩ‰∏ç‰º†Â≠óÊÆµ**„ÄÇ
> Â¶ÇÊûú‰Ω†ÊÉ≥ËÆ©Â≠óÊÆµÂÆåÂÖ®ÂèØÈÄâÔºàÂèØ‰∏ç‰º†ÔºâÔºåÂ∫îÂÜôÊàêÔºö`request_id: Optional[str] = None`

`request_text: Union[str, List[str]] = Field(..., description="ËØ∑Ê±ÇÊñáÊú¨„ÄÅÂ≠óÁ¨¶‰∏≤ÊàñÂàóË°®")`

* Á±ªÂûãÔºöÂèØ‰ª•ÊòØ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤Ôºå‰πüÂèØ‰ª•ÊòØ‰∏Ä‰∏™Â≠óÁ¨¶‰∏≤ÂàóË°®„ÄÇ
  * ÊØîÂ¶ÇÔºö`"‰ªäÂ§©Â§©Ê∞îÁúüÂ•Ω"` Êàñ `["‰ªäÂ§©Â§©Ê∞îÁúüÂ•Ω", "ÊàëÂæàÂºÄÂøÉ"]`
* ÂøÖÂ°´Â≠óÊÆµÔºàÂõ†‰∏∫Áî®‰∫Ü `...`Ôºâ
* Áî®ÈÄîÔºöË°®Á§∫Ë¶ÅËøõË°åÂàÜÁ±ªÁöÑÊñáÊú¨ÂÜÖÂÆπÔºåÊîØÊåÅÂçïÊù°ÊàñÊâπÈáèËæìÂÖ•„ÄÇ

```python
# ÂìçÂ∫îÊ®°Âûã
class TextClassifyResponse(BaseModel):
    """
    Êé•Âè£ËøîÂõûÊ†ºÂºè
    """
    request_id: Optional[str] = Field(..., description="ËØ∑Ê±Çid")
    request_text: Union[str, List[str]] = Field(..., description="ËØ∑Ê±ÇÊñáÊú¨„ÄÅÂ≠óÁ¨¶‰∏≤ÊàñÂàóË°®")
    classify_result: Union[str, List[str]] = Field(..., description="ÂàÜÁ±ªÁªìÊûú")
    classify_time: float = Field(..., description="ÂàÜÁ±ªËÄóÊó∂")
    error_msg: str = Field(..., description="ÂºÇÂ∏∏‰ø°ÊÅØ")
```

`request_id: Optional[str] = Field(...)`

* ËøîÂõûÂÆ¢Êà∑Á´Ø‰º†ÂÖ•ÁöÑ `request_id`ÔºåÊñπ‰æøÂØπÂ∫îËØ∑Ê±Ç‰∏éÂìçÂ∫î„ÄÇ
* ‰ªçÊòØÂèØ‰∏∫Á©∫ÁöÑÂ≠óÁ¨¶‰∏≤Ôºå‰∏î‰∏∫ÂøÖÂ°´Â≠óÊÆµÔºàÂøÖÈ°ªËøîÂõûÔºå‰ΩÜÂÄºÂèØ‰ª•ÊòØ `null`Ôºâ„ÄÇ

`request_text: Union[str, List[str]] = Field(...)`

* ÂõûÊòæÂÆ¢Êà∑Á´Ø‰º†ÂÖ•ÁöÑÂéüÂßãÊñáÊú¨Ôºå‰æø‰∫éÊ†∏ÂØπ„ÄÇ
* Á±ªÂûã‰∏éËØ∑Ê±Ç‰∏ÄËá¥„ÄÇ

`classify_result: Union[str, List[str]] = Field(...)`

* ÂàÜÁ±ªÁªìÊûúÔºåÂ¶ÇÊûúËæìÂÖ•ÊòØÂ≠óÁ¨¶‰∏≤ÔºåËæìÂá∫Â∞±ÊòØÂçï‰∏™ÂàÜÁ±ªÊ†áÁ≠æÔºàÂ¶Ç `"Ë¥üÈù¢"`ÔºâÔºõ
* Â¶ÇÊûúËæìÂÖ•ÊòØÂàóË°®ÔºåËæìÂá∫‰πüÂ∫îÊòØÂØπÂ∫îÁöÑÊ†áÁ≠æÂàóË°®ÔºàÂ¶Ç `["Ë¥üÈù¢", "Ê≠£Èù¢"]`Ôºâ„ÄÇ
* Á±ªÂûã‰∏é `request_text` ÂØπÂ∫î„ÄÇ

`classify_time: float = Field(...)`

* Á±ªÂûãÔºöÊµÆÁÇπÊï∞ÔºàÂçï‰ΩçÔºöÁßíÔºâ
* Ë°®Á§∫Ê®°ÂûãÂÆåÊàêÂàÜÁ±ªÊâÄËä±Ë¥πÁöÑÊó∂Èó¥ÔºåÁî®‰∫éÊÄßËÉΩÁõëÊéßÊàñÂâçÁ´ØÂ±ïÁ§∫„ÄÇ
* ‰æãÂ¶ÇÔºö`0.123` Ë°®Á§∫ËÄóÊó∂ 123 ÊØ´Áßí„ÄÇ

`error_msg: str = Field(...)`

* ÈîôËØØ‰ø°ÊÅØÂ≠óÊÆµ„ÄÇ
* Âç≥‰ΩøÊàêÂäüÔºå‰πüÂª∫ËÆÆËøîÂõûÁ©∫Â≠óÁ¨¶‰∏≤ `""` Êàñ `"success"`„ÄÇ
* Â¶ÇÊûúÂá∫ÈîôÔºåÂ°´ÂÖÖÈîôËØØÊèèËø∞ÔºåÂ¶Ç `"Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•"`„ÄÇ



### Êé®ÁêÜÂáΩÊï∞

```python
# ÂØºÂÖ•‰æùËµñÂíåÂàùÂßãÂåñÈÖçÁΩÆ
from typing import Union, List
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForSequenceClassification

from config import BERT_MODEL_PERTRAINED_PATH, BERT_MODEL_PKL_PATH, CATEGORY_NAME

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PERTRAINED_PATH)
model = BertForSequenceClassification.from_pretrained(BERT_MODEL_PERTRAINED_PATH, num_labels=2)
model.to(device)
```

Ëá™ÂÆö‰πâdatasetsÁ±ª

```python
class NewsDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(int(self.labels[idx]))
        return item

    def __len__(self):
        return len(self.labels)
```


ËøôÊòØ PyTorch ÁöÑÊ†áÂáÜÊï∞ÊçÆÈõÜÂ∞ÅË£ÖÁ±ªÔºåÁî®‰∫éÂ∞ÜÊñáÊú¨ÁºñÁ†ÅÂíåÊ†áÁ≠æÊâìÂåÖ‰æõ `DataLoader` ‰ΩøÁî®„ÄÇ

ÊñπÊ≥ïËØ¶Ëß£Ôºö

* `__init__`: Êé•Êî∂ `encodings`Ôºàtokenize ÂêéÁöÑÁªìÊûúÔºâÂíå `labels`ÔºàÊ†áÁ≠æÂàóË°®Ôºâ„ÄÇ
* `__getitem__(idx)`:
  * ÊääÊØè‰∏™Ê†∑Êú¨ÁöÑ `input_ids`, `attention_mask` Á≠âËΩ¨‰∏∫ `torch.tensor`„ÄÇ
  * ÂêåÊó∂ÊääÊ†áÁ≠æ‰πüËΩ¨‰∏∫ tensorÔºàËôΩÁÑ∂ÊòØÊµãËØïÈõÜÔºå‰ΩÜ‰∏∫‰∫ÜÁªü‰∏ÄËæìÂÖ•Ê†ºÂºè‰ªçÈúÄÊèê‰æõ `labels`Ôºâ„ÄÇ
* `__len__()`: ËøîÂõûÊ†∑Êú¨Êï∞Èáè„ÄÇ

> üí° Ê≥®ÊÑèÔºöÊµãËØïÊó∂ `labels` ËÆæ‰∏∫ `[0]*len(request_text)` ÊòØÂêàÁêÜÁöÑÔºåÂõ†‰∏∫Êàë‰ª¨Âè™ÂÖ≥ÂøÉÈ¢ÑÊµãÁªìÊûúÔºå‰∏çÂèÇ‰∏éÊçüÂ§±ËÆ°ÁÆó„ÄÇ


Ê†∏ÂøÉÂáΩÊï∞

```python
def model_for_bert(request_text: Union[str, List[str]]) -> Union[str, List[str]]:
    classify_result: Union[str, List[str]] = None

    if isinstance(request_text, str):
        request_text = [request_text]
    elif isinstance(request_text, list):
        pass
    else:
        raise Exception("Ê†ºÂºè‰∏çÊîØÊåÅ")

    test_encoding = tokenizer(list(request_text), truncation=True, padding=True, max_length=30)
    test_dataset = NewsDataset(test_encoding, [0] * len(request_text))
    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)

    model.eval()
    pred = []
    for batch in test_dataloader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs[1]
        logits = logits.detach().cpu().numpy()
        pred += list(np.argmax(logits, axis=1).flatten())

    classify_result = [CATEGORY_NAME[x] for x in pred]
    return classify_result
```
