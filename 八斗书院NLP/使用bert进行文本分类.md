# ä½¿ç”¨bertè¿›è¡Œæ–‡æœ¬åˆ†ç±»

## å¤–å–è¯„è®ºæ•°æ®é›†

```
label,review
1,å¾ˆå¿«ï¼Œå¥½åƒï¼Œå‘³é“è¶³ï¼Œé‡å¤§
1,æ²¡æœ‰é€æ°´æ²¡æœ‰é€æ°´æ²¡æœ‰é€æ°´
1,éå¸¸å¿«ï¼Œæ€åº¦å¥½ã€‚
1,æ–¹ä¾¿ï¼Œå¿«æ·ï¼Œå‘³é“å¯å£ï¼Œå¿«é€’ç»™åŠ›
1,èœå‘³é“å¾ˆæ£’ï¼é€é¤å¾ˆåŠæ—¶ï¼
1,ä»Šå¤©å¸ˆå‚…æ˜¯ä¸æ˜¯æ‰‹æŠ–äº†ï¼Œå¾®è¾£æ ¼å¤–è¾£ï¼
1,"é€é¤å¿«,æ€åº¦ä¹Ÿç‰¹åˆ«å¥½,è¾›è‹¦å•¦è°¢è°¢"
```

## bertå¾®è°ƒ

```python
# å¯¼å…¥å¿…è¦çš„åº“
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
```

```python
# åŠ è½½æ•°æ®é›†
dataset_path = "waimai_10k.csv"

dataset_df = pd.read_csv(dataset_path, sep=",", header=None)[1:]

print(dataset_df.shape)
print(dataset_df.head())
```

```python
# åˆå§‹åŒ– LabelEncoderï¼Œç”¨äºå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
lbl = LabelEncoder()
# æ‹Ÿåˆæ•°æ®å¹¶è½¬æ¢å‰500ä¸ªæ ‡ç­¾ï¼Œå¾—åˆ°æ•°å­—æ ‡ç­¾
labels = lbl.fit_transform(dataset_df[0].values[:])
texts = list(dataset_df[1].values[:])

print(len(texts))

# åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
x_train, x_test, train_labels, test_labels = train_test_split(
    texts,             # æ–‡æœ¬æ•°æ®
    labels,            # å¯¹åº”çš„æ•°å­—æ ‡ç­¾
    test_size=0.2,     # æµ‹è¯•é›†æ¯”ä¾‹ä¸º20%
    stratify=labels    # ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒä¸€è‡´
)
```

```python
# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('./bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('./bert-base-chinese', num_labels=2)
```

```python
# ä½¿ç”¨åˆ†è¯å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–‡æœ¬è¿›è¡Œç¼–ç 
# truncation=Trueï¼šå¦‚æœæ–‡æœ¬è¿‡é•¿åˆ™æˆªæ–­
# padding=Trueï¼šå¯¹é½æ‰€æœ‰åºåˆ—é•¿åº¦ï¼Œå¡«å……åˆ°æœ€é•¿
# max_length=64ï¼šæœ€å¤§åºåˆ—é•¿åº¦
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=64)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=64)
```

```python
# å°†ç¼–ç åçš„æ•°æ®å’Œæ ‡ç­¾è½¬æ¢ä¸º Hugging Face `datasets` åº“çš„ Dataset å¯¹è±¡
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],           # æ–‡æœ¬çš„token ID
    'attention_mask': train_encodings['attention_mask'], # æ³¨æ„åŠ›æ©ç 
    'labels': train_labels                               # å¯¹åº”çš„æ ‡ç­¾
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels
})

print(train_dataset.shape)
```

```python
# å®šä¹‰ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def compute_metrics(eval_pred):
    # eval_pred æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡å‹é¢„æµ‹çš„ logits å’ŒçœŸå®çš„æ ‡ç­¾
    logits, labels = eval_pred
    # æ‰¾åˆ° logits ä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„ç±»åˆ«
    predictions = np.argmax(logits, axis=-1)
    # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡å¹¶è¿”å›ä¸€ä¸ªå­—å…¸
    return {'accuracy': (predictions == labels).mean()}
```

```python
# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',              # è®­ç»ƒè¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜æ¨¡å‹å’ŒçŠ¶æ€
    num_train_epochs=8,                  # è®­ç»ƒçš„æ€»è½®æ•°
    per_device_train_batch_size=32,      # è®­ç»ƒæ—¶æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰çš„æ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=32,       # è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
    warmup_steps=500,                    # å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
    weight_decay=0.01,                   # æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    logging_dir='./logs',                # æ—¥å¿—å­˜å‚¨ç›®å½•
    logging_steps=100,                   # æ¯éš”100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    eval_strategy="epoch",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    save_strategy="best",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    load_best_model_at_end=True,         # è®­ç»ƒç»“æŸååŠ è½½æ•ˆæœæœ€å¥½çš„æ¨¡å‹
)

# å®ä¾‹åŒ– Trainer
trainer = Trainer(
    model=model,                         # è¦è®­ç»ƒçš„æ¨¡å‹
    args=training_args,                  # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,         # è®­ç»ƒæ•°æ®é›†
    eval_dataset=test_dataset,           # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,     # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
)

# å¼€å§‹è®­ç»ƒæ¨¡å‹
trainer.train()
# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
trainer.evaluate()
trainer.save_model("best")
print("Done")
```

## fastapiéƒ¨ç½²

### æ•°æ®æ¥å£å®šä¹‰

```python
from pydantic import BaseModel, Field
from typing import Dict, List, Any, Union, Optional
```

BaseModel: pydantic çš„æ ¸å¿ƒç±»ï¼Œæ‰€æœ‰æ•°æ®æ¨¡å‹éƒ½ç»§æ‰¿è‡ªå®ƒï¼Œç”¨äºå®šä¹‰ç»“æ„åŒ–æ•°æ®ã€‚
Field: ç”¨äºå¯¹å­—æ®µæ·»åŠ é¢å¤–ä¿¡æ¯ï¼Œå¦‚é»˜è®¤å€¼ã€æè¿°ã€çº¦æŸç­‰ã€‚
Dict, List, Any, Union, Optional: æ¥è‡ª typing æ¨¡å—ï¼Œç”¨äºç±»å‹æ³¨è§£ï¼š

* Optional[str] ç­‰ä»·äº Union[str, None]ï¼Œè¡¨ç¤ºè¯¥å­—æ®µå¯ä¸ºç©ºï¼ˆå³å¯ä»¥æ˜¯ str æˆ– Noneï¼‰ã€‚
  Union[A, B] è¡¨ç¤ºå­—æ®µå¯ä»¥æ˜¯ A ç±»å‹æˆ– B ç±»å‹ã€‚
  List[str] è¡¨ç¤ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
  Any è¡¨ç¤ºä»»æ„ç±»å‹ï¼ˆä¸æ¨èè¿‡åº¦ä½¿ç”¨ï¼Œä¼šå¤±å»ç±»å‹å®‰å…¨ï¼‰ã€‚

```python
# è¯·æ±‚æ¨¡å‹
class TextClassifyRequest(BaseModel):
    """
    è¯·æ±‚æ ¼å¼
    """
    request_id: Optional[str] = Field(..., description="è¯·æ±‚id, æ–¹ä¾¿è°ƒè¯•")
    request_text: Union[str, List[str]] = Field(..., description="è¯·æ±‚æ–‡æœ¬ã€å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")
```

`request_id: Optional[str] = Field(..., description="è¯·æ±‚id, æ–¹ä¾¿è°ƒè¯•")`

* ç±»å‹ï¼š`Optional[str]` â†’ å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ `None`ï¼ˆå³è¿™ä¸ªå­—æ®µä¸æ˜¯å¿…é¡»ä¼ çš„ï¼‰ã€‚
* `Field(..., ...)`ï¼š
  * ç¬¬ä¸€ä¸ª `...` è¡¨ç¤ºè¿™ä¸ªå­—æ®µæ˜¯**å¿…å¡«é¡¹**ï¼ˆå³ä½¿ç±»å‹æ˜¯ `Optional`ï¼Œä½†å¦‚æœä¸ä¼ å€¼ï¼Œä¹Ÿä¼šæŠ¥é”™ï¼‰ã€‚
  * `description`ï¼šå­—æ®µæè¿°ï¼Œä¼šåœ¨ API æ–‡æ¡£ï¼ˆå¦‚ Swaggerï¼‰ä¸­æ˜¾ç¤ºã€‚
* ç”¨é€”ï¼šå®¢æˆ·ç«¯ä¼ ä¸€ä¸ªè¯·æ±‚ IDï¼Œä¾¿äºæœåŠ¡ç«¯æ—¥å¿—è¿½è¸ªå’Œè°ƒè¯•ã€‚

> âš ï¸ æ³¨æ„ï¼š`Optional[str]` + `...` æ„å‘³ç€ï¼š**å¯ä»¥ä¼  nullï¼Œä½†ä¸èƒ½ä¸ä¼ å­—æ®µ**ã€‚
> å¦‚æœä½ æƒ³è®©å­—æ®µå®Œå…¨å¯é€‰ï¼ˆå¯ä¸ä¼ ï¼‰ï¼Œåº”å†™æˆï¼š`request_id: Optional[str] = None`

`request_text: Union[str, List[str]] = Field(..., description="è¯·æ±‚æ–‡æœ¬ã€å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")`

* ç±»å‹ï¼šå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
  * æ¯”å¦‚ï¼š`"ä»Šå¤©å¤©æ°”çœŸå¥½"` æˆ– `["ä»Šå¤©å¤©æ°”çœŸå¥½", "æˆ‘å¾ˆå¼€å¿ƒ"]`
* å¿…å¡«å­—æ®µï¼ˆå› ä¸ºç”¨äº† `...`ï¼‰
* ç”¨é€”ï¼šè¡¨ç¤ºè¦è¿›è¡Œåˆ†ç±»çš„æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒå•æ¡æˆ–æ‰¹é‡è¾“å…¥ã€‚

```python
# å“åº”æ¨¡å‹
class TextClassifyResponse(BaseModel):
    """
    æ¥å£è¿”å›æ ¼å¼
    """
    request_id: Optional[str] = Field(..., description="è¯·æ±‚id")
    request_text: Union[str, List[str]] = Field(..., description="è¯·æ±‚æ–‡æœ¬ã€å­—ç¬¦ä¸²æˆ–åˆ—è¡¨")
    classify_result: Union[str, List[str]] = Field(..., description="åˆ†ç±»ç»“æœ")
    classify_time: float = Field(..., description="åˆ†ç±»è€—æ—¶")
    error_msg: str = Field(..., description="å¼‚å¸¸ä¿¡æ¯")
```

`request_id: Optional[str] = Field(...)`

* è¿”å›å®¢æˆ·ç«¯ä¼ å…¥çš„ `request_id`ï¼Œæ–¹ä¾¿å¯¹åº”è¯·æ±‚ä¸å“åº”ã€‚
* ä»æ˜¯å¯ä¸ºç©ºçš„å­—ç¬¦ä¸²ï¼Œä¸”ä¸ºå¿…å¡«å­—æ®µï¼ˆå¿…é¡»è¿”å›ï¼Œä½†å€¼å¯ä»¥æ˜¯ `null`ï¼‰ã€‚

`request_text: Union[str, List[str]] = Field(...)`

* å›æ˜¾å®¢æˆ·ç«¯ä¼ å…¥çš„åŸå§‹æ–‡æœ¬ï¼Œä¾¿äºæ ¸å¯¹ã€‚
* ç±»å‹ä¸è¯·æ±‚ä¸€è‡´ã€‚

`classify_result: Union[str, List[str]] = Field(...)`

* åˆ†ç±»ç»“æœï¼Œå¦‚æœè¾“å…¥æ˜¯å­—ç¬¦ä¸²ï¼Œè¾“å‡ºå°±æ˜¯å•ä¸ªåˆ†ç±»æ ‡ç­¾ï¼ˆå¦‚ `"è´Ÿé¢"`ï¼‰ï¼›
* å¦‚æœè¾“å…¥æ˜¯åˆ—è¡¨ï¼Œè¾“å‡ºä¹Ÿåº”æ˜¯å¯¹åº”çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå¦‚ `["è´Ÿé¢", "æ­£é¢"]`ï¼‰ã€‚
* ç±»å‹ä¸ `request_text` å¯¹åº”ã€‚

`classify_time: float = Field(...)`

* ç±»å‹ï¼šæµ®ç‚¹æ•°ï¼ˆå•ä½ï¼šç§’ï¼‰
* è¡¨ç¤ºæ¨¡å‹å®Œæˆåˆ†ç±»æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œç”¨äºæ€§èƒ½ç›‘æ§æˆ–å‰ç«¯å±•ç¤ºã€‚
* ä¾‹å¦‚ï¼š`0.123` è¡¨ç¤ºè€—æ—¶ 123 æ¯«ç§’ã€‚

`error_msg: str = Field(...)`

* é”™è¯¯ä¿¡æ¯å­—æ®µã€‚
* å³ä½¿æˆåŠŸï¼Œä¹Ÿå»ºè®®è¿”å›ç©ºå­—ç¬¦ä¸² `""` æˆ– `"success"`ã€‚
* å¦‚æœå‡ºé”™ï¼Œå¡«å……é”™è¯¯æè¿°ï¼Œå¦‚ `"æ¨¡å‹åŠ è½½å¤±è´¥"`ã€‚



### æ¨ç†å‡½æ•°

```python
# å¯¼å…¥ä¾èµ–å’Œåˆå§‹åŒ–é…ç½®
from typing import Union, List
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

from transformers import AutoTokenizer, AutoModelForMaskedLM, BertForSequenceClassification

from config import BERT_MODEL_PERTRAINED_PATH, BERT_MODEL_PKL_PATH, CATEGORY_NAME

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_PERTRAINED_PATH)
model = BertForSequenceClassification.from_pretrained(BERT_MODEL_PERTRAINED_PATH, num_labels=2)
model.to(device)
```

è‡ªå®šä¹‰datasetsç±»

```python
class NewsDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(int(self.labels[idx]))
        return item

    def __len__(self):
        return len(self.labels)
```


è¿™æ˜¯ PyTorch çš„æ ‡å‡†æ•°æ®é›†å°è£…ç±»ï¼Œç”¨äºå°†æ–‡æœ¬ç¼–ç å’Œæ ‡ç­¾æ‰“åŒ…ä¾› `DataLoader` ä½¿ç”¨ã€‚

æ–¹æ³•è¯¦è§£ï¼š

* `__init__`: æ¥æ”¶ `encodings`ï¼ˆtokenize åçš„ç»“æœï¼‰å’Œ `labels`ï¼ˆæ ‡ç­¾åˆ—è¡¨ï¼‰ã€‚
* `__getitem__(idx)`:
  * æŠŠæ¯ä¸ªæ ·æœ¬çš„ `input_ids`, `attention_mask` ç­‰è½¬ä¸º `torch.tensor`ã€‚
  * åŒæ—¶æŠŠæ ‡ç­¾ä¹Ÿè½¬ä¸º tensorï¼ˆè™½ç„¶æ˜¯æµ‹è¯•é›†ï¼Œä½†ä¸ºäº†ç»Ÿä¸€è¾“å…¥æ ¼å¼ä»éœ€æä¾› `labels`ï¼‰ã€‚
* `__len__()`: è¿”å›æ ·æœ¬æ•°é‡ã€‚

> ğŸ’¡ æ³¨æ„ï¼šæµ‹è¯•æ—¶ `labels` è®¾ä¸º `[0]*len(request_text)` æ˜¯åˆç†çš„ï¼Œå› ä¸ºæˆ‘ä»¬åªå…³å¿ƒé¢„æµ‹ç»“æœï¼Œä¸å‚ä¸æŸå¤±è®¡ç®—ã€‚


æ ¸å¿ƒå‡½æ•°

```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥å¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¾“å‡ºæ˜¯å¯¹åº”çš„åˆ†ç±»ç»“æœï¼ˆå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰ã€‚
def model_for_bert(request_text: Union[str, List[str]]) -> Union[str, List[str]]:
    classify_result: Union[str, List[str]] = None
#   è¾“å…¥æ ¼å¼ç»Ÿä¸€åŒ–
    if isinstance(request_text, str):
        request_text = [request_text]
    elif isinstance(request_text, list):
        pass
    else:
        raise Exception("æ ¼å¼ä¸æ”¯æŒ")
# åˆ†è¯ä¸ç¼–ç 
# å‚æ•°è¯´æ˜ï¼š
#    truncation=True: è¶…è¿‡æœ€å¤§é•¿åº¦æ—¶æˆªæ–­ã€‚
#    padding=True: è‡ªåŠ¨è¡¥å…¨åˆ° batch ä¸­æœ€é•¿åºåˆ—é•¿åº¦ï¼ˆç”¨äº batch æ¨ç†ï¼‰ã€‚
#    max_length=30: æœ€å¤šä¿ç•™ 30 ä¸ª tokenï¼ˆåŒ…å« [CLS], [SEP]ï¼‰ã€‚
    test_encoding = tokenizer(list(request_text), truncation=True, padding=True, max_length=30)
  
#   æ„å»ºæµ‹è¯•æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    test_dataset = NewsDataset(test_encoding, [0] * len(request_text))
    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)
#   æ¨¡å‹æ¨ç†
    model.eval()
    pred = []
    for batch in test_dataloader:
        with torch.no_grad():
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        logits = outputs[1]
        logits = logits.detach().cpu().numpy()
        pred += list(np.argmax(logits, axis=1).flatten())
#   æ˜ å°„ç±»åˆ«åç§°
    classify_result = [CATEGORY_NAME[x] for x in pred]
    return classify_result
```

æ¨¡å‹æ¨ç†éƒ¨åˆ†

* `model.eval()`: åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropoutã€batch norm ä½¿ç”¨å›ºå®šç»Ÿè®¡é‡ï¼‰ã€‚
* `with torch.no_grad()`: åœæ­¢æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ã€‚
* `input_ids`, `attention_mask`, `labels` ç§»åŠ¨åˆ°è®¾å¤‡ä¸Šï¼ˆGPU/CPUï¼‰ã€‚
* `outputs = model(...)`:
  * è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œ`outputs[0]` æ˜¯ lossï¼ˆå› ä¸ºæä¾›äº† labelsï¼‰ï¼Œ`outputs[1]` æ˜¯ logitsã€‚
  * æˆ‘ä»¬åªéœ€è¦ `logits`ï¼ˆæœªå½’ä¸€åŒ–çš„åˆ†ç±»å¾—åˆ†ï¼‰ã€‚
* `logits.detach().cpu().numpy()`:
  * ä»è®¡ç®—å›¾ä¸­åˆ†ç¦» â†’ ç§»åŠ¨åˆ° CPU â†’ è½¬ä¸º NumPy æ•°ç»„ã€‚
* `np.argmax(..., axis=1)`: æ‰¾å‡ºæ¯æ¡æ ·æœ¬å¾—åˆ†æœ€é«˜çš„ç±»åˆ«ç´¢å¼•ã€‚
* `pred += list(...)`ï¼šå°†å½“å‰ batch çš„é¢„æµ‹ç»“æœåŠ å…¥æ€»åˆ—è¡¨ã€‚


## è¿™é‡Œä¸ºä»€ä¹ˆè¦æ„å»ºæµ‹è¯•æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼Ÿ

**æ„å»º `Dataset` + `DataLoader` æ˜¯ä¸ºäº†ï¼š**

1. **ç»Ÿä¸€è®­ç»ƒä¸æ¨ç†çš„æ•°æ®æµç¨‹**
2. **æ”¯æŒæ‰¹é‡æ¨ç†ï¼ˆbatch inferenceï¼‰ï¼Œæå‡æ•ˆç‡**
3. **åˆ©ç”¨ PyTorch çš„è‡ªåŠ¨æ‰¹å¤„ç†å’Œè®¾å¤‡æ¬è¿æœºåˆ¶**
4. **ä¿è¯ä»£ç ç»“æ„æ¸…æ™°ã€å¯ç»´æŠ¤ã€å¯æ‰©å±•**




| âœ… æ‰¹é‡æ¨ç†     | æå‡ GPU åˆ©ç”¨ç‡ï¼ŒåŠ å¿«é€Ÿåº¦ |
| --------------- | ------------------------- |
| âœ… å†…å­˜å‹å¥½     | é¿å…ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®    |
| âœ… æµç¨‹ç»Ÿä¸€     | è®­ç»ƒã€éªŒè¯ã€æ¨ç†ä¿æŒä¸€è‡´  |
| âœ… è‡ªåŠ¨ padding | ä¸ç”¨æ‰‹åŠ¨å¯¹é½åºåˆ—é•¿åº¦      |
| âœ… æ˜“äºç»´æŠ¤     | ç»“æ„æ¸…æ™°ï¼Œä¾¿äºæ‰©å±•å’Œè°ƒè¯•  |
| âœ… å·¥ç¨‹æœ€ä½³å®è·µ | å·¥ä¸šçº§é¡¹ç›®çš„æ ‡å‡†åšæ³•      |
