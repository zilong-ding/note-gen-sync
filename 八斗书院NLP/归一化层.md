# pytorch中的归一化层

## Batch Normalization

### ✅ 核心思想：

对一个 batch 中的所有样本，在**每个通道（channel）上**，沿着 **batch 维度和空间维度（H×W）** 进行归一化。

### 📐 公式：

对于一个 mini-batch 的数据，形状为 `(N, C, H, W)`：

对每个通道 `c`，计算该通道在所有样本和空间位置上的均值和方差：

$$
μ_c = mean(x_{n,c,h,w})
$$

$$
σ²_c = var(x_{n,c,h,w})
$$

$$
x̂_{n,c,h,w} = (x_{n,c,h,w} - μ_c) / √(σ²\_c + ε)
$$

$$
y_{n,c,h,w} = γ_c * x̂_{n,c,h,w} + β_c
$$

### ✅ 优点：

* 显著加速训练收敛。
* 允许使用更高的学习率。
* 有一定的正则化效果（类似 Dropout）。
* 广泛用于 CNN。

### ❌ 缺点：

* **依赖 batch size**：小 batch 时统计不稳定（如 batch=1 时完全失效）。
* **不适合 RNN/Transformer**：序列长度不固定，batch 内样本结构不同。
* 推理时需用滑动平均统计量，训练/推理行为不一致。

### 🎯 适用场景：

图像分类、目标检测等 CNN 任务，batch size 较大时。

### 为什么要在channel上进行归一化

#### 🎯 1. 核心动机：通道是语义特征的载体

在 CNN 中，**每个通道（channel）通常代表一种特定的语义特征响应**，比如：

* 第1个通道：检测“边缘”
* 第5个通道：检测“红色区域”
* 第20个通道：检测“车轮形状”

这些通道的数值分布（均值、方差）是**完全不同**的。比如边缘检测通道可能输出值集中在 [-1, 1]，而颜色通道可能集中在 [0, 255]（经过缩放后）。

✅ **如果对所有通道一起归一化**（比如把 C×H×W 所有值拉平一起算均值方差），就会：

* **抹平不同语义特征之间的差异**。
* 强行让“边缘响应”和“颜色响应”具有相同的均值和方差 → 破坏网络学到的语义结构。
* 导致性能下降（实验验证过）。

📌 所以，**必须按通道独立归一化**，保留每个通道自身的语义特性，只消除该通道内部的分布偏移。

---

#### 🧮 2. 数学合理性：通道内共享参数 → 应共享统计量

在卷积层中，**同一个通道的所有空间位置（H×W）共享同一组卷积核参数**。也就是说：

* 同一个通道的每个位置，都是用**同一个 filter** 计算出来的。
* 所以，它们理应具有**相似的数值分布特性**。

✅ 因此，在一个 batch 内，对**同一个通道的所有空间位置和所有样本**（即 N×H×W）计算均值和方差，是统计上合理的 —— 它们是“同分布”的。

❌ 如果跨通道归一化，等于把不同分布的数据（不同 filter 输出）强行拉到一起，违反了统计独立性假设。

---

#### 🏗️ 3. 网络结构兼容性：保持通道独立性

现代 CNN 架构（如 ResNet、VGG）中，通道之间是**并行处理、独立演化**的。后续的 1x1 卷积、注意力机制、通道注意力（如 SENet）等模块，都依赖于**通道维度的独立性**。

✅ BatchNorm 按通道归一化，正好与这种结构兼容：

* 每个通道有自己的 γ 和 β（可学习参数）→ 可以独立缩放/偏移。
* 后续层可以基于“归一化后的通道”做特征选择或加权。

---

#### 📊 4. 实验验证：跨通道归一化效果差

原始 BatchNorm 论文（Ioffe & Szegedy, 2015）中其实做过对比实验：

* 作者尝试过 “**Layer-wise normalization**”，即对每个样本的所有通道一起归一化（类似 LayerNorm）。
* 结果：**训练更慢、精度更低**。

> 原文：“We could have normalized all activations in a layer, but since the different features... may have different scales, such normalization could lose important information.”

翻译：我们本可以归一化一层中所有激活值，但由于不同特征可能具有不同尺度，这种归一化会丢失重要信息。



## Layer Normalization (LayerNorm)



## Instance Normalization (InstanceNorm)




## Group Normalization (GroupNorm)
