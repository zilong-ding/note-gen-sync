# pytorch中的归一化层

## Batch Normalization

$$
y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta 
$$

$$
\begin{aligned}&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是仿射参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是BatchNorm2D层仅有的可学习}\\&\text{参数。}\end{aligned}
$$

### ✅ 核心思想：

对一个 batch 中的所有样本，在**每个通道（channel）上**，沿着 **batch 维度和空间维度（H×W）** 进行归一化。

### 📐 公式：

对于一个 mini-batch 的数据，形状为 `(N, C, H, W)`：

对每个通道 `c`，计算该通道在所有样本和空间位置上的均值和方差：

$$
μ_c = mean(x_{n,c,h,w})
$$

$$
σ²_c = var(x_{n,c,h,w})
$$

$$
x̂_{n,c,h,w} = (x_{n,c,h,w} - μ_c) / √(σ²\_c + ε)
$$

$$
y_{n,c,h,w} = γ_c * x̂_{n,c,h,w} + β_c
$$

### ✅ 优点：

* 显著加速训练收敛。
* 允许使用更高的学习率。
* 有一定的正则化效果（类似 Dropout）。
* 广泛用于 CNN。

### ❌ 缺点：

* **依赖 batch size**：小 batch 时统计不稳定（如 batch=1 时完全失效）。
* **不适合 RNN/Transformer**：序列长度不固定，batch 内样本结构不同。
* 推理时需用滑动平均统计量，训练/推理行为不一致。

### 🎯 适用场景：

图像分类、目标检测等 CNN 任务，batch size 较大时。

### 为什么要在channel上进行归一化

#### 🎯 1. 核心动机：通道是语义特征的载体

在 CNN 中，**每个通道（channel）通常代表一种特定的语义特征响应**，比如：

* 第1个通道：检测“边缘”
* 第5个通道：检测“红色区域”
* 第20个通道：检测“车轮形状”

这些通道的数值分布（均值、方差）是**完全不同**的。比如边缘检测通道可能输出值集中在 [-1, 1]，而颜色通道可能集中在 [0, 255]（经过缩放后）。

✅ **如果对所有通道一起归一化**（比如把 C×H×W 所有值拉平一起算均值方差），就会：

* **抹平不同语义特征之间的差异**。
* 强行让“边缘响应”和“颜色响应”具有相同的均值和方差 → 破坏网络学到的语义结构。
* 导致性能下降（实验验证过）。

📌 所以，**必须按通道独立归一化**，保留每个通道自身的语义特性，只消除该通道内部的分布偏移。

---

#### 🧮 2. 数学合理性：通道内共享参数 → 应共享统计量

在卷积层中，**同一个通道的所有空间位置（H×W）共享同一组卷积核参数**。也就是说：

* 同一个通道的每个位置，都是用**同一个 filter** 计算出来的。
* 所以，它们理应具有**相似的数值分布特性**。

✅ 因此，在一个 batch 内，对**同一个通道的所有空间位置和所有样本**（即 N×H×W）计算均值和方差，是统计上合理的 —— 它们是“同分布”的。

❌ 如果跨通道归一化，等于把不同分布的数据（不同 filter 输出）强行拉到一起，违反了统计独立性假设。

---

#### 🏗️ 3. 网络结构兼容性：保持通道独立性

现代 CNN 架构（如 ResNet、VGG）中，通道之间是**并行处理、独立演化**的。后续的 1x1 卷积、注意力机制、通道注意力（如 SENet）等模块，都依赖于**通道维度的独立性**。

✅ BatchNorm 按通道归一化，正好与这种结构兼容：

* 每个通道有自己的 γ 和 β（可学习参数）→ 可以独立缩放/偏移。
* 后续层可以基于“归一化后的通道”做特征选择或加权。

---

#### 📊 4. 实验验证：跨通道归一化效果差

原始 BatchNorm 论文（Ioffe & Szegedy, 2015）中其实做过对比实验：

* 作者尝试过 “**Layer-wise normalization**”，即对每个样本的所有通道一起归一化（类似 LayerNorm）。
* 结果：**训练更慢、精度更低**。

> 原文：“We could have normalized all activations in a layer, but since the different features... may have different scales, such normalization could lose important information.”

翻译：我们本可以归一化一层中所有激活值，但由于不同特征可能具有不同尺度，这种归一化会丢失重要信息。



## Layer Normalization (LayerNorm)

$$
\begin{aligned}&y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathbf{Var}[x]+\epsilon}}*\gamma+\beta\\&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是信行参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是LayerNorm层仅有的可学习参}\\&\text{数。}\end{aligned}
$$

### ✅ 核心思想：

对**单个样本**，在**所有特征维度**上做归一化（不跨样本）。常用于 NLP 和 RNN。

### 📐 公式：

对每个样本 `n`，计算其所有特征（如 `(C, H, W)` 展平后的所有值）的均值和方差：

$$
μ\_n = mean(x\_{n, :})  // 对 c, h, w 求平均
$$

$$
σ²\_n = var(x\_{n, :})
$$

$$
x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_n) / √(σ²\_n + ε)
$$

$$
y\_{n,c,h,w} = γ * x̂\_{n,c,h,w} + β   // γ, β 是可学习参数（通常 per-feature）
$$

> 注意：在 Transformer 中，是对每个 token 的 embedding 维度做 LayerNorm，即 `(B, T, D)` → 对 `D` 维度归一化。

### ✅ 优点：

* **不依赖 batch size** → 适合小 batch、在线学习、RNN。
* 在 Transformer 中成为标配（如 BERT、GPT）。
* 训练与推理行为一致。

### ❌ 缺点：

* 在 CNN 中效果通常不如 BatchNorm（因为破坏了通道间的统计特性）。
* 对图像的空间结构不敏感。

### 🎯 适用场景：

Transformer、RNN、小 batch 场景、NLP 任务。

## 为什么要对

#### 🎯 1. 核心动机：NLP / Transformer 中“特征维度没有独立语义”

在 NLP 任务中（如 Transformer、BERT、GPT），一个样本通常是一个序列，形状为：

1

**(**Batch, Sequence Length, Feature Dimension**)** = (B, T, D)

* `D` 是 embedding 维度（如 768），它**不是一个“通道”概念**，而是**一个稠密的语义向量空间**。
* 每个维度（d ∈ [1, D]）**没有独立语义** —— 你不能说“第5维代表名词”，“第300维代表情感”。
* 整个向量 `x ∈ ℝᴰ` 一起表达一个 token 的语义（如 “cat” 的 embedding）。

✅ 所以，**对整个 D 维向量做归一化是合理的** —— 它是一个“语义整体”。

> ❗ 如果你像 BatchNorm 那样“per-dimension”归一化（即对每个 d，在 B×T 上统计），反而会破坏向量内部的相对结构，且在推理时 batch=1 会崩溃。

---

#### 🧮 2. 数学合理性：稳定注意力机制的输入分布

Transformer 的核心是 **Self-Attention**：

Attention(Q, K, V) = softmax(QKᵀ / √d\_k) V

* Q、K、V 是从同一个输入 `X ∈ (B, T, D)` 线性变换得来。
* 如果 `X` 的每个向量（每个 token 的 embedding）**长度（L2 norm）差异很大**，会导致：
  * softmax 输入值过大 → 梯度消失（某些位置 attention=1，其余=0）
  * 训练不稳定、收敛慢。

✅ LayerNorm 对每个 token 向量独立归一化，使得：

* 每个 token 的 embedding 向量具有**相似的尺度（均值0，方差1）**。
* 注意力计算更稳定，梯度更平滑。

📌 **LayerNorm 是“per-token”归一化，不是“per-dimension”归一化** —— 这是关键！

---

#### 🏗️ 3. 结构兼容性：适用于变长序列、小 batch、RNN

LayerNorm 的最大优势是：**不依赖 batch 维度**。

* 对每个样本（甚至每个 token）独立计算统计量 → batch size = 1 也能用。
* RNN 中每个时间步输入长度不确定 → BatchNorm 无法处理。
* Transformer 中不同样本序列长度不同 → BatchNorm 难以对齐。

✅ LayerNorm 对每个 token 的 D 维向量归一化，天然支持：

* 变长序列
* 在线学习
* 小 batch / 单样本推理

## Instance Normalization (InstanceNorm)




## Group Normalization (GroupNorm)
