# pytorch中的归一化层

## Batch Normalization

### ✅ 核心思想：

对一个 batch 中的所有样本，在**每个通道（channel）上**，沿着 **batch 维度和空间维度（H×W）** 进行归一化。

### 📐 公式：

对于一个 mini-batch 的数据，形状为 `(N, C, H, W)`：

对每个通道 `c`，计算该通道在所有样本和空间位置上的均值和方差：

1

2

3

4

μ\_c = mean(x\_{n,c,h,w})  // 对 n, h, w 求平均

σ²\_c = var(x\_{n,c,h,w})

x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_c) / √(σ²\_c + ε)

y\_{n,c,h,w} = γ\_c \* x̂\_{n,c,h,w} + β\_c   // 可学习的缩放和平移参数

### ✅ 优点：

* 显著加速训练收敛。
* 允许使用更高的学习率。
* 有一定的正则化效果（类似 Dropout）。
* 广泛用于 CNN。

### ❌ 缺点：

* **依赖 batch size**：小 batch 时统计不稳定（如 batch=1 时完全失效）。
* **不适合 RNN/Transformer**：序列长度不固定，batch 内样本结构不同。
* 推理时需用滑动平均统计量，训练/推理行为不一致。

### 🎯 适用场景：

图像分类、目标检测等 CNN 任务，batch size 较大时。

### 为什么要在channel上进行归一化
