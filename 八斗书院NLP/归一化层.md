# pytorch中的归一化层

## Batch Normalization

$$
y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta 
$$

$$
\begin{aligned}&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是仿射参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是BatchNorm2D层仅有的可学习}\\&\text{参数。}\end{aligned}
$$

### ✅ 核心思想：

对一个 batch 中的所有样本，在**每个通道（channel）上**，沿着 **batch 维度和空间维度（H×W）** 进行归一化。

### 📐 公式：

对于一个 mini-batch 的数据，形状为 `(N, C, H, W)`：

对每个通道 `c`，计算该通道在所有样本和空间位置上的均值和方差：

$$
μ_c = mean(x_{n,c,h,w})
$$

$$
σ²_c = var(x_{n,c,h,w})
$$

$$
x̂_{n,c,h,w} = (x_{n,c,h,w} - μ_c) / √(σ²\_c + ε)
$$

$$
y_{n,c,h,w} = γ_c * x̂_{n,c,h,w} + β_c
$$

### ✅ 优点：

* 显著加速训练收敛。
* 允许使用更高的学习率。
* 有一定的正则化效果（类似 Dropout）。
* 广泛用于 CNN。

### ❌ 缺点：

* **依赖 batch size**：小 batch 时统计不稳定（如 batch=1 时完全失效）。
* **不适合 RNN/Transformer**：序列长度不固定，batch 内样本结构不同。
* 推理时需用滑动平均统计量，训练/推理行为不一致。

### 🎯 适用场景：

图像分类、目标检测等 CNN 任务，batch size 较大时。

### 为什么要在channel上进行归一化

#### 🎯 1. 核心动机：通道是语义特征的载体

在 CNN 中，**每个通道（channel）通常代表一种特定的语义特征响应**，比如：

* 第1个通道：检测“边缘”
* 第5个通道：检测“红色区域”
* 第20个通道：检测“车轮形状”

这些通道的数值分布（均值、方差）是**完全不同**的。比如边缘检测通道可能输出值集中在 [-1, 1]，而颜色通道可能集中在 [0, 255]（经过缩放后）。

✅ **如果对所有通道一起归一化**（比如把 C×H×W 所有值拉平一起算均值方差），就会：

* **抹平不同语义特征之间的差异**。
* 强行让“边缘响应”和“颜色响应”具有相同的均值和方差 → 破坏网络学到的语义结构。
* 导致性能下降（实验验证过）。

📌 所以，**必须按通道独立归一化**，保留每个通道自身的语义特性，只消除该通道内部的分布偏移。

---

#### 🧮 2. 数学合理性：通道内共享参数 → 应共享统计量

在卷积层中，**同一个通道的所有空间位置（H×W）共享同一组卷积核参数**。也就是说：

* 同一个通道的每个位置，都是用**同一个 filter** 计算出来的。
* 所以，它们理应具有**相似的数值分布特性**。

✅ 因此，在一个 batch 内，对**同一个通道的所有空间位置和所有样本**（即 N×H×W）计算均值和方差，是统计上合理的 —— 它们是“同分布”的。

❌ 如果跨通道归一化，等于把不同分布的数据（不同 filter 输出）强行拉到一起，违反了统计独立性假设。

---

#### 🏗️ 3. 网络结构兼容性：保持通道独立性

现代 CNN 架构（如 ResNet、VGG）中，通道之间是**并行处理、独立演化**的。后续的 1x1 卷积、注意力机制、通道注意力（如 SENet）等模块，都依赖于**通道维度的独立性**。

✅ BatchNorm 按通道归一化，正好与这种结构兼容：

* 每个通道有自己的 γ 和 β（可学习参数）→ 可以独立缩放/偏移。
* 后续层可以基于“归一化后的通道”做特征选择或加权。

---

#### 📊 4. 实验验证：跨通道归一化效果差

原始 BatchNorm 论文（Ioffe & Szegedy, 2015）中其实做过对比实验：

* 作者尝试过 “**Layer-wise normalization**”，即对每个样本的所有通道一起归一化（类似 LayerNorm）。
* 结果：**训练更慢、精度更低**。

> 原文：“We could have normalized all activations in a layer, but since the different features... may have different scales, such normalization could lose important information.”

翻译：我们本可以归一化一层中所有激活值，但由于不同特征可能具有不同尺度，这种归一化会丢失重要信息。



## Layer Normalization (LayerNorm)

$$
\begin{aligned}&y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathbf{Var}[x]+\epsilon}}*\gamma+\beta\\&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是信行参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是LayerNorm层仅有的可学习参}\\&\text{数。}\end{aligned}
$$

### ✅ 核心思想：

对**单个样本**，在**所有特征维度**上做归一化（不跨样本）。常用于 NLP 和 RNN。

### 📐 公式：

对每个样本 `n`，计算其所有特征（如 `(C, H, W)` 展平后的所有值）的均值和方差：

$$
μ\_n = mean(x\_{n, :})  // 对 c, h, w 求平均
$$

$$
σ²\_n = var(x\_{n, :})
$$

$$
x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_n) / √(σ²\_n + ε)
$$

$$
y\_{n,c,h,w} = γ * x̂\_{n,c,h,w} + β   // γ, β 是可学习参数（通常 per-feature）
$$

> 注意：在 Transformer 中，是对每个 token 的 embedding 维度做 LayerNorm，即 `(B, T, D)` → 对 `D` 维度归一化。

### ✅ 优点：

* **不依赖 batch size** → 适合小 batch、在线学习、RNN。
* 在 Transformer 中成为标配（如 BERT、GPT）。
* 训练与推理行为一致。

### ❌ 缺点：

* 在 CNN 中效果通常不如 BatchNorm（因为破坏了通道间的统计特性）。
* 对图像的空间结构不敏感。

### 🎯 适用场景：

Transformer、RNN、小 batch 场景、NLP 任务。


## Instance Normalization (InstanceNorm)




## Group Normalization (GroupNorm)
