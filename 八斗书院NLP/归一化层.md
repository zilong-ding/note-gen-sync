# pytorch中的归一化层

## Batch Normalization

$$
y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta 
$$

$$
\begin{aligned}&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是仿射参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是BatchNorm2D层仅有的可学习}\\&\text{参数。}\end{aligned}
$$

### ✅ 核心思想：

对一个 batch 中的所有样本，在**每个通道（channel）上**，沿着 **batch 维度和空间维度（H×W）** 进行归一化。

### 📐 公式：

对于一个 mini-batch 的数据，形状为 `(N, C, H, W)`：

对每个通道 `c`，计算该通道在所有样本和空间位置上的均值和方差：

$$
μ_c = mean(x_{n,c,h,w})
$$

$$
σ²_c = var(x_{n,c,h,w})
$$

$$
x̂_{n,c,h,w} = (x_{n,c,h,w} - μ_c) / √(σ²\_c + ε)
$$

$$
y_{n,c,h,w} = γ_c * x̂_{n,c,h,w} + β_c
$$

### ✅ 优点：

* 显著加速训练收敛。
* 允许使用更高的学习率。
* 有一定的正则化效果（类似 Dropout）。
* 广泛用于 CNN。

### ❌ 缺点：

* **依赖 batch size**：小 batch 时统计不稳定（如 batch=1 时完全失效）。
* **不适合 RNN/Transformer**：序列长度不固定，batch 内样本结构不同。
* 推理时需用滑动平均统计量，训练/推理行为不一致。

### 🎯 适用场景：

图像分类、目标检测等 CNN 任务，batch size 较大时。

### 为什么要在channel上进行归一化

#### 🎯 1. 核心动机：通道是语义特征的载体

在 CNN 中，**每个通道（channel）通常代表一种特定的语义特征响应**，比如：

* 第1个通道：检测“边缘”
* 第5个通道：检测“红色区域”
* 第20个通道：检测“车轮形状”

这些通道的数值分布（均值、方差）是**完全不同**的。比如边缘检测通道可能输出值集中在 [-1, 1]，而颜色通道可能集中在 [0, 255]（经过缩放后）。

✅ **如果对所有通道一起归一化**（比如把 C×H×W 所有值拉平一起算均值方差），就会：

* **抹平不同语义特征之间的差异**。
* 强行让“边缘响应”和“颜色响应”具有相同的均值和方差 → 破坏网络学到的语义结构。
* 导致性能下降（实验验证过）。

📌 所以，**必须按通道独立归一化**，保留每个通道自身的语义特性，只消除该通道内部的分布偏移。

---

#### 🧮 2. 数学合理性：通道内共享参数 → 应共享统计量

在卷积层中，**同一个通道的所有空间位置（H×W）共享同一组卷积核参数**。也就是说：

* 同一个通道的每个位置，都是用**同一个 filter** 计算出来的。
* 所以，它们理应具有**相似的数值分布特性**。

✅ 因此，在一个 batch 内，对**同一个通道的所有空间位置和所有样本**（即 N×H×W）计算均值和方差，是统计上合理的 —— 它们是“同分布”的。

❌ 如果跨通道归一化，等于把不同分布的数据（不同 filter 输出）强行拉到一起，违反了统计独立性假设。

---

#### 🏗️ 3. 网络结构兼容性：保持通道独立性

现代 CNN 架构（如 ResNet、VGG）中，通道之间是**并行处理、独立演化**的。后续的 1x1 卷积、注意力机制、通道注意力（如 SENet）等模块，都依赖于**通道维度的独立性**。

✅ BatchNorm 按通道归一化，正好与这种结构兼容：

* 每个通道有自己的 γ 和 β（可学习参数）→ 可以独立缩放/偏移。
* 后续层可以基于“归一化后的通道”做特征选择或加权。

---

#### 📊 4. 实验验证：跨通道归一化效果差

原始 BatchNorm 论文（Ioffe & Szegedy, 2015）中其实做过对比实验：

* 作者尝试过 “**Layer-wise normalization**”，即对每个样本的所有通道一起归一化（类似 LayerNorm）。
* 结果：**训练更慢、精度更低**。

> 原文：“We could have normalized all activations in a layer, but since the different features... may have different scales, such normalization could lose important information.”

翻译：我们本可以归一化一层中所有激活值，但由于不同特征可能具有不同尺度，这种归一化会丢失重要信息。



## Layer Normalization (LayerNorm)

$$
\begin{aligned}&y=\frac{x-\mathbf{E}[x]}{\sqrt{\mathbf{Var}[x]+\epsilon}}*\gamma+\beta\\&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是信行参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是可学习的。可以发现,}\gamma\text{和 }\beta\text{是LayerNorm层仅有的可学习参}\\&\text{数。}\end{aligned}
$$

### ✅ 核心思想：

对**单个样本**，在**所有特征维度**上做归一化（不跨样本）。常用于 NLP 和 RNN。

### 📐 公式：

对每个样本 `n`，计算其所有特征（如 `(C, H, W)` 展平后的所有值）的均值和方差：

$$
μ\_n = mean(x\_{n, :})  // 对 c, h, w 求平均
$$

$$
σ²\_n = var(x\_{n, :})
$$

$$
x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_n) / √(σ²\_n + ε)
$$

$$
y\_{n,c,h,w} = γ * x̂\_{n,c,h,w} + β   // γ, β 是可学习参数（通常 per-feature）
$$

> 注意：在 Transformer 中，是对每个 token 的 embedding 维度做 LayerNorm，即 `(B, T, D)` → 对 `D` 维度归一化。

### ✅ 优点：

* **不依赖 batch size** → 适合小 batch、在线学习、RNN。
* 在 Transformer 中成为标配（如 BERT、GPT）。
* 训练与推理行为一致。

### ❌ 缺点：

* 在 CNN 中效果通常不如 BatchNorm（因为破坏了通道间的统计特性）。
* 对图像的空间结构不敏感。

### 🎯 适用场景：

Transformer、RNN、小 batch 场景、NLP 任务。

### 为什么要对所有特征维度上做归一化

#### 🎯 1. 核心动机：NLP / Transformer 中“特征维度没有独立语义”

在 NLP 任务中（如 Transformer、BERT、GPT），一个样本通常是一个序列，形状为：

**(**Batch, Sequence Length, Feature Dimension**)** = (B, T, D)

* `D` 是 embedding 维度（如 768），它**不是一个“通道”概念**，而是**一个稠密的语义向量空间**。
* 每个维度（d ∈ [1, D]）**没有独立语义** —— 你不能说“第5维代表名词”，“第300维代表情感”。
* 整个向量 `x ∈ ℝᴰ` 一起表达一个 token 的语义（如 “cat” 的 embedding）。

✅ 所以，**对整个 D 维向量做归一化是合理的** —— 它是一个“语义整体”。

> ❗ 如果你像 BatchNorm 那样“per-dimension”归一化（即对每个 d，在 B×T 上统计），反而会破坏向量内部的相对结构，且在推理时 batch=1 会崩溃。

---

#### 🧮 2. 数学合理性：稳定注意力机制的输入分布

Transformer 的核心是 **Self-Attention**：

Attention(Q, K, V) = softmax(QKᵀ / √d\_k) V

* Q、K、V 是从同一个输入 `X ∈ (B, T, D)` 线性变换得来。
* 如果 `X` 的每个向量（每个 token 的 embedding）**长度（L2 norm）差异很大**，会导致：
  * softmax 输入值过大 → 梯度消失（某些位置 attention=1，其余=0）
  * 训练不稳定、收敛慢。

✅ LayerNorm 对每个 token 向量独立归一化，使得：

* 每个 token 的 embedding 向量具有**相似的尺度（均值0，方差1）**。
* 注意力计算更稳定，梯度更平滑。

📌 **LayerNorm 是“per-token”归一化，不是“per-dimension”归一化** —— 这是关键！

---

#### 🏗️ 3. 结构兼容性：适用于变长序列、小 batch、RNN

LayerNorm 的最大优势是：**不依赖 batch 维度**。

* 对每个样本（甚至每个 token）独立计算统计量 → batch size = 1 也能用。
* RNN 中每个时间步输入长度不确定 → BatchNorm 无法处理。
* Transformer 中不同样本序列长度不同 → BatchNorm 难以对齐。

✅ LayerNorm 对每个 token 的 D 维向量归一化，天然支持：

* 变长序列
* 在线学习
* 小 batch / 单样本推理

## Instance Normalization (InstanceNorm)

$$
\begin{aligned}&y=\frac{z-\mathbf{E}[x]}{\sqrt{\mathbf{Var}[x]+\epsilon}}*\gamma+\beta\\&\text{其中前一项是归一化过程。分母中的 }\epsilon\text{ 是一个非常小的数,作用是防止数值计算不稳定。}\gamma\\&\text{和 }\beta\text{ 是仿射参数,将归一化后的数据再次放缩得到新的数据,}\gamma\text{可以理解为标准差,}\beta\text{可以}\\&\text{理解为均值,它们两个一般是不可学习的。}\end{aligned}
$$


### ✅ 核心思想：

对**每个样本的每个通道**，独立在**空间维度（H×W）** 上做归一化。常用于风格迁移。

### 📐 公式：

对每个样本 `n` 和每个通道 `c`：

μ\_{n,c} = mean(x\_{n,c,h,w})  // 仅对 h, w 求平均

σ²\_{n,c} = var(x\_{n,c,h,w})

x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_{n,c}) / √(σ²\_{n,c} + ε)

y\_{n,c,h,w} = γ\_c \* x̂\_{n,c,h,w} + β\_c

### ✅ 优点：

* 去除图像的对比度信息 → 保留“内容结构”，适合风格迁移。
* 不依赖 batch，单样本即可归一化。
* 在生成任务（如 GAN、风格迁移）中表现优异。

### ❌ 缺点：

* 丢弃了 batch 内统计信息 → 不适合分类等需要全局语义的任务。
* 在判别式任务中通常不如 BatchNorm。

### 🎯 适用场景：

图像风格迁移、图像生成（GAN）、图像到图像翻译。


### 为什么instancenorm 对每个样本的每个通道，独立在空间维度（H×W） 上做归一化?

#### 🎯 1. 核心动机：风格迁移任务中，要“去除内容对比度，保留空间结构”

InstanceNorm 最初是为 **图像风格迁移（Neural Style Transfer）** 设计的（Ulyanov et al., CVPR 2017）。

在风格迁移中：

* **内容图像** → 提供“空间结构/形状”（如一只猫的轮廓）
* **风格图像** → 提供“纹理、颜色、笔触”等统计特征

✅ InstanceNorm 的作用是：

> **对每个通道（代表某种纹理/语义响应），在空间维度（H×W）上归一化 → 抹去该通道内的“对比度/亮度变化”，只保留“空间激活模式” → 即“内容结构”**

举个例子：

* 假设某个通道负责“检测毛发纹理”，原始图像中猫背部亮、腹部暗 → 有强对比度。
* InstanceNorm 会把这个通道的所有空间位置归一化 → 亮部变中性，暗部也变中性 → 对比度消失。
* 但“哪里有毛发”（空间位置）仍然保留 → 结构还在！

📌 **所以，InstanceNorm 的本质是：保留空间结构（where），去除局部对比度（how strong）→ 让网络更容易“贴上”新的风格。**

---

#### 🧮 2. 数学合理性：每个通道的空间响应应具有“局部一致性”

在 CNN 中，一个通道的 feature map 可以看作：

> “某种视觉模式在图像不同位置的响应强度图”

比如：

* 边缘检测通道 → 在边缘处值高，平坦区域值低
* 红色响应通道 → 在红色区域值高

✅ 对这个响应图（H×W）做归一化：

* 均值 = 该通道在整个图像上的“平均激活强度”
* 方差 = 该通道激活的“动态范围/对比度”

归一化后：

* 所有通道的响应图都被“拉平”到均值0、方差1
* **消除了图像的“局部对比度”和“亮度偏移”**
* 但**空间激活模式（结构）仍然保留**

这正是风格迁移需要的：**结构保留，风格重写**。

---

#### 🖼️ 3. 视觉特性：图像风格 = 通道内空间统计量

在风格迁移中，“风格”通常被定义为**Gram 矩阵**或**通道内特征的相关性统计**（如均值、方差、协方差）。

* InstanceNorm 显式地**移除了每个通道的空间均值和方差**
* 后续风格注入层（如 AdaIN、仿射变换）可以**重新注入目标风格的均值和方差**

👉 这就是著名的 **AdaIN（Adaptive Instance Normalization）** 的核心：

AdaIN(x, style) = style\_scale \* (x - μ(x)) / σ(x) + style\_bias

* `μ(x), σ(x)` 是 InstanceNorm 统计量（per channel per sample）
* `style_scale, style_bias` 来自风格图像（或学习得到）

✅ 所以，InstanceNorm 是“风格可控”的基础 —— 它把内容和风格（统计量）解耦了！


## Group Normalization (GroupNorm)


### ✅ 核心思想：

将通道分成若干组（group），对**每个样本的每个组**，在**组内通道 + 空间维度**上做归一化。是 BatchNorm 和 LayerNorm 的折中。

### 📐 公式：

假设将 C 个通道分为 G 组（每组有 C/G 个通道），对每个样本 `n` 和每组 `g`：

μ\_{n,g} = mean(x\_{n, c∈g, h, w})

σ²\_{n,g} = var(x\_{n, c∈g, h, w})

x̂\_{n,c,h,w} = (x\_{n,c,h,w} - μ\_{n,g}) / √(σ²\_{n,g} + ε)

y\_{n,c,h,w} = γ\_c \* x̂\_{n,c,h,w} + β\_c

> 当 G=1 → LayerNorm（整个通道一起归一化）
> 当 G=C → InstanceNorm（每个通道独立归一化）
> 当 G 适中（如 32）→ GroupNorm

### ✅ 优点：

* **不依赖 batch size** → 小 batch 也能稳定训练。
* 在检测、分割等任务中表现优于 BatchNorm（尤其 batch size 小时）。
* 保留了一定的通道间统计信息。

### ❌ 缺点：

* 需要手动设置组数 G（通常设为 32 效果较好）。
* 在大 batch CNN 分类任务中略逊于 BatchNorm。

### 🎯 适用场景：

目标检测（如 Mask R-CNN）、语义分割、小 batch 训练、视频任务。


### 为什么groupnorm对每个样本的每个组，在组内通道 + 空间维度上做归一化，有什么依据呢？


#### 🎯 1. 核心动机：解决 BatchNorm 对 batch size 的依赖

GroupNorm 是何恺明团队在 2018 年提出的（ECCV），**直接动机是解决 BatchNorm 在小 batch 场景下的性能崩溃问题**：

* BatchNorm 在 batch size < 16 时统计不稳定 → 检测/分割任务常用小 batch（因显存限制）→ 性能下降。
* InstanceNorm 不跨通道 → 丢弃太多语义信息 → 在判别任务（如检测）中表现差。
* LayerNorm 跨所有通道 → 破坏 CNN 的通道语义结构 → 在图像任务中不如 BN。

✅ GroupNorm 的设计目标：

> **不依赖 batch 维度，同时保留通道间的部分统计信息，折中 InstanceNorm 和 LayerNorm，在小 batch 下媲美甚至超越 BatchNorm。**

---

#### 🧩 2. 设计依据一：通道可以分组 —— 语义相关性假设

在 CNN 中，虽然每个通道代表不同语义（如边缘、纹理、颜色），但**相邻或相近的通道往往语义相关**：

* 前几个通道可能都在检测“不同方向的边缘”
* 中间通道可能都在响应“某种纹理模式”
* 后面通道可能组合成“高级语义部件”

✅ 所以，**把 C 个通道分成 G 组（如 G=32），每组包含 C/G 个通道，假设组内通道“语义相似” → 可以共享归一化统计量**。

👉 这是一种“局部语义一致性”假设 —— 比 LayerNorm（全部通道一起）更合理，比 InstanceNorm（每个通道独立）保留更多信息。

📌 举例：

假设你有 64 个通道，分成 8 组，每组 8 个通道：

* 第1组：8个“边缘方向响应”通道 → 一起归一化合理
* 第2组：8个“红色系纹理”通道 → 一起归一化合理

> 这类似于“局部注意力”或“分组卷积”的思想 —— 局部相关，全局独立。

---

#### 🧮 3. 设计依据二：数学与统计合理性

对每个样本 `n`，每个组 `g`，计算统计量的维度是：

**(**C/G, H, W**)** → 拉平成一个大向量 → 计算均值和方差

✅ 为什么合理？

* **空间维度 H×W**：同一通道内空间位置共享卷积核 → 响应值分布相似 → 应一起归一化（和 BN/IN 一致）。
* **组内通道 C/G**：假设语义相关 → 响应值分布近似 → 可共享统计量。
* **不跨样本**：避免 batch 依赖 → 单样本也能归一化。

这相当于：

> “在一个语义组内，把所有空间响应值看作一个‘局部特征集合’，对其进行标准化。”

---

#### 🧪 4. 实验依据：G=32 是“经验最优值”，对 C 不敏感

论文中做了大量实验，发现：

* **G=32 时效果最好**，且对总通道数 C 不敏感（C=64, 128, 256... 效果稳定）。
* G=1 → 等价于 LayerNorm（效果差）
* G=C → 等价于 InstanceNorm（效果差于 GN）
* G=32 在检测、分割、分类任务中全面超越 IN/LN，小 batch 下超越 BN。

📌 作者说：

> “Surprisingly, GN’s accuracy is stable across a wide range of group numbers (from 2 to 64), and 32 groups work well in practice.”

→ 说明“通道可分组”这个假设是成立的，且**32 是一个对多数 CNN 结构都鲁棒的经验值**。

---

## 🖼️ 5. 与人类视觉/神经科学的类比（启发式解释）

人类视觉皮层中，神经元是“分组组织”的：

* 初级视觉皮层（V1）：神经元按“方向选择性”分组（类似边缘检测通道分组）
* 高级区域：神经元按“物体部件”或“语义类别”分组

✅ GroupNorm 的“分组归一化”可以看作是对这种**生物神经分组处理机制的粗略模拟**：

> “在局部语义组内做归一化，避免全局干扰，保留组内一致性。”
