# pytorchä¸­çš„å½’ä¸€åŒ–å±‚

## Batch Normalization

### âœ… æ ¸å¿ƒæ€æƒ³ï¼š

å¯¹ä¸€ä¸ª batch ä¸­çš„æ‰€æœ‰æ ·æœ¬ï¼Œåœ¨**æ¯ä¸ªé€šé“ï¼ˆchannelï¼‰ä¸Š**ï¼Œæ²¿ç€ **batch ç»´åº¦å’Œç©ºé—´ç»´åº¦ï¼ˆHÃ—Wï¼‰** è¿›è¡Œå½’ä¸€åŒ–ã€‚

### ğŸ“ å…¬å¼ï¼š

å¯¹äºä¸€ä¸ª mini-batch çš„æ•°æ®ï¼Œå½¢çŠ¶ä¸º `(N, C, H, W)`ï¼š

å¯¹æ¯ä¸ªé€šé“ `c`ï¼Œè®¡ç®—è¯¥é€šé“åœ¨æ‰€æœ‰æ ·æœ¬å’Œç©ºé—´ä½ç½®ä¸Šçš„å‡å€¼å’Œæ–¹å·®ï¼š

$$
Î¼_c = mean(x_{n,c,h,w})
$$

$$
ÏƒÂ²_c = var(x_{n,c,h,w})
$$

$$
xÌ‚_{n,c,h,w} = (x_{n,c,h,w} - Î¼_c) / âˆš(ÏƒÂ²\_c + Îµ)
$$

$$
y_{n,c,h,w} = Î³_c * xÌ‚_{n,c,h,w} + Î²_c
$$

### âœ… ä¼˜ç‚¹ï¼š

* æ˜¾è‘—åŠ é€Ÿè®­ç»ƒæ”¶æ•›ã€‚
* å…è®¸ä½¿ç”¨æ›´é«˜çš„å­¦ä¹ ç‡ã€‚
* æœ‰ä¸€å®šçš„æ­£åˆ™åŒ–æ•ˆæœï¼ˆç±»ä¼¼ Dropoutï¼‰ã€‚
* å¹¿æ³›ç”¨äº CNNã€‚

### âŒ ç¼ºç‚¹ï¼š

* **ä¾èµ– batch size**ï¼šå° batch æ—¶ç»Ÿè®¡ä¸ç¨³å®šï¼ˆå¦‚ batch=1 æ—¶å®Œå…¨å¤±æ•ˆï¼‰ã€‚
* **ä¸é€‚åˆ RNN/Transformer**ï¼šåºåˆ—é•¿åº¦ä¸å›ºå®šï¼Œbatch å†…æ ·æœ¬ç»“æ„ä¸åŒã€‚
* æ¨ç†æ—¶éœ€ç”¨æ»‘åŠ¨å¹³å‡ç»Ÿè®¡é‡ï¼Œè®­ç»ƒ/æ¨ç†è¡Œä¸ºä¸ä¸€è‡´ã€‚

### ğŸ¯ é€‚ç”¨åœºæ™¯ï¼š

å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ CNN ä»»åŠ¡ï¼Œbatch size è¾ƒå¤§æ—¶ã€‚

### ä¸ºä»€ä¹ˆè¦åœ¨channelä¸Šè¿›è¡Œå½’ä¸€åŒ–

#### ğŸ¯ 1. æ ¸å¿ƒåŠ¨æœºï¼šé€šé“æ˜¯è¯­ä¹‰ç‰¹å¾çš„è½½ä½“

åœ¨ CNN ä¸­ï¼Œ**æ¯ä¸ªé€šé“ï¼ˆchannelï¼‰é€šå¸¸ä»£è¡¨ä¸€ç§ç‰¹å®šçš„è¯­ä¹‰ç‰¹å¾å“åº”**ï¼Œæ¯”å¦‚ï¼š

* ç¬¬1ä¸ªé€šé“ï¼šæ£€æµ‹â€œè¾¹ç¼˜â€
* ç¬¬5ä¸ªé€šé“ï¼šæ£€æµ‹â€œçº¢è‰²åŒºåŸŸâ€
* ç¬¬20ä¸ªé€šé“ï¼šæ£€æµ‹â€œè½¦è½®å½¢çŠ¶â€

è¿™äº›é€šé“çš„æ•°å€¼åˆ†å¸ƒï¼ˆå‡å€¼ã€æ–¹å·®ï¼‰æ˜¯**å®Œå…¨ä¸åŒ**çš„ã€‚æ¯”å¦‚è¾¹ç¼˜æ£€æµ‹é€šé“å¯èƒ½è¾“å‡ºå€¼é›†ä¸­åœ¨ [-1, 1]ï¼Œè€Œé¢œè‰²é€šé“å¯èƒ½é›†ä¸­åœ¨ [0, 255]ï¼ˆç»è¿‡ç¼©æ”¾åï¼‰ã€‚

âœ… **å¦‚æœå¯¹æ‰€æœ‰é€šé“ä¸€èµ·å½’ä¸€åŒ–**ï¼ˆæ¯”å¦‚æŠŠ CÃ—HÃ—W æ‰€æœ‰å€¼æ‹‰å¹³ä¸€èµ·ç®—å‡å€¼æ–¹å·®ï¼‰ï¼Œå°±ä¼šï¼š

* **æŠ¹å¹³ä¸åŒè¯­ä¹‰ç‰¹å¾ä¹‹é—´çš„å·®å¼‚**ã€‚
* å¼ºè¡Œè®©â€œè¾¹ç¼˜å“åº”â€å’Œâ€œé¢œè‰²å“åº”â€å…·æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·® â†’ ç ´åç½‘ç»œå­¦åˆ°çš„è¯­ä¹‰ç»“æ„ã€‚
* å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼ˆå®éªŒéªŒè¯è¿‡ï¼‰ã€‚

ğŸ“Œ æ‰€ä»¥ï¼Œ**å¿…é¡»æŒ‰é€šé“ç‹¬ç«‹å½’ä¸€åŒ–**ï¼Œä¿ç•™æ¯ä¸ªé€šé“è‡ªèº«çš„è¯­ä¹‰ç‰¹æ€§ï¼Œåªæ¶ˆé™¤è¯¥é€šé“å†…éƒ¨çš„åˆ†å¸ƒåç§»ã€‚

---

#### ğŸ§® 2. æ•°å­¦åˆç†æ€§ï¼šé€šé“å†…å…±äº«å‚æ•° â†’ åº”å…±äº«ç»Ÿè®¡é‡

åœ¨å·ç§¯å±‚ä¸­ï¼Œ**åŒä¸€ä¸ªé€šé“çš„æ‰€æœ‰ç©ºé—´ä½ç½®ï¼ˆHÃ—Wï¼‰å…±äº«åŒä¸€ç»„å·ç§¯æ ¸å‚æ•°**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼š

* åŒä¸€ä¸ªé€šé“çš„æ¯ä¸ªä½ç½®ï¼Œéƒ½æ˜¯ç”¨**åŒä¸€ä¸ª filter** è®¡ç®—å‡ºæ¥çš„ã€‚
* æ‰€ä»¥ï¼Œå®ƒä»¬ç†åº”å…·æœ‰**ç›¸ä¼¼çš„æ•°å€¼åˆ†å¸ƒç‰¹æ€§**ã€‚

âœ… å› æ­¤ï¼Œåœ¨ä¸€ä¸ª batch å†…ï¼Œå¯¹**åŒä¸€ä¸ªé€šé“çš„æ‰€æœ‰ç©ºé—´ä½ç½®å’Œæ‰€æœ‰æ ·æœ¬**ï¼ˆå³ NÃ—HÃ—Wï¼‰è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼Œæ˜¯ç»Ÿè®¡ä¸Šåˆç†çš„ â€”â€” å®ƒä»¬æ˜¯â€œåŒåˆ†å¸ƒâ€çš„ã€‚

âŒ å¦‚æœè·¨é€šé“å½’ä¸€åŒ–ï¼Œç­‰äºæŠŠä¸åŒåˆ†å¸ƒçš„æ•°æ®ï¼ˆä¸åŒ filter è¾“å‡ºï¼‰å¼ºè¡Œæ‹‰åˆ°ä¸€èµ·ï¼Œè¿åäº†ç»Ÿè®¡ç‹¬ç«‹æ€§å‡è®¾ã€‚

---

#### ğŸ—ï¸ 3. ç½‘ç»œç»“æ„å…¼å®¹æ€§ï¼šä¿æŒé€šé“ç‹¬ç«‹æ€§

ç°ä»£ CNN æ¶æ„ï¼ˆå¦‚ ResNetã€VGGï¼‰ä¸­ï¼Œé€šé“ä¹‹é—´æ˜¯**å¹¶è¡Œå¤„ç†ã€ç‹¬ç«‹æ¼”åŒ–**çš„ã€‚åç»­çš„ 1x1 å·ç§¯ã€æ³¨æ„åŠ›æœºåˆ¶ã€é€šé“æ³¨æ„åŠ›ï¼ˆå¦‚ SENetï¼‰ç­‰æ¨¡å—ï¼Œéƒ½ä¾èµ–äº**é€šé“ç»´åº¦çš„ç‹¬ç«‹æ€§**ã€‚

âœ… BatchNorm æŒ‰é€šé“å½’ä¸€åŒ–ï¼Œæ­£å¥½ä¸è¿™ç§ç»“æ„å…¼å®¹ï¼š

* æ¯ä¸ªé€šé“æœ‰è‡ªå·±çš„ Î³ å’Œ Î²ï¼ˆå¯å­¦ä¹ å‚æ•°ï¼‰â†’ å¯ä»¥ç‹¬ç«‹ç¼©æ”¾/åç§»ã€‚
* åç»­å±‚å¯ä»¥åŸºäºâ€œå½’ä¸€åŒ–åçš„é€šé“â€åšç‰¹å¾é€‰æ‹©æˆ–åŠ æƒã€‚

---

#### ğŸ“Š 4. å®éªŒéªŒè¯ï¼šè·¨é€šé“å½’ä¸€åŒ–æ•ˆæœå·®

åŸå§‹ BatchNorm è®ºæ–‡ï¼ˆIoffe & Szegedy, 2015ï¼‰ä¸­å…¶å®åšè¿‡å¯¹æ¯”å®éªŒï¼š

* ä½œè€…å°è¯•è¿‡ â€œ**Layer-wise normalization**â€ï¼Œå³å¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰é€šé“ä¸€èµ·å½’ä¸€åŒ–ï¼ˆç±»ä¼¼ LayerNormï¼‰ã€‚
* ç»“æœï¼š**è®­ç»ƒæ›´æ…¢ã€ç²¾åº¦æ›´ä½**ã€‚

> åŸæ–‡ï¼šâ€œWe could have normalized all activations in a layer, but since the different features... may have different scales, such normalization could lose important information.â€

ç¿»è¯‘ï¼šæˆ‘ä»¬æœ¬å¯ä»¥å½’ä¸€åŒ–ä¸€å±‚ä¸­æ‰€æœ‰æ¿€æ´»å€¼ï¼Œä½†ç”±äºä¸åŒç‰¹å¾å¯èƒ½å…·æœ‰ä¸åŒå°ºåº¦ï¼Œè¿™ç§å½’ä¸€åŒ–ä¼šä¸¢å¤±é‡è¦ä¿¡æ¯ã€‚



## Layer Normalization (LayerNorm)



## Instance Normalization (InstanceNorm)




## Group Normalization (GroupNorm)
