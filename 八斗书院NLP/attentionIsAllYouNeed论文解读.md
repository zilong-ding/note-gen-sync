# AttentionIsAllYouNeed论文解读

## 背景

RNN,LSTM,GRU是当时比较序列建模，语言建模和机器翻译中最先进的方法，但是这三种方法都是属于串行结构。串行结构问题在于并行效率低和层数过多是会出现信息丢失的现象。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许在忽略输入或输出序列中依赖关系距离的情况下进行建模2,19]。然而，在除少数情况27]之外的所有情况下，此类注意力机制都与循环网络结合使用。

在这个工作中，我们提出了Transformer,一种摒弃递归并完全依赖注意力机制来在输入和输出之间建立全局依赖关系的模型架构。Transformer允许实现更高级别的并行化，在八个Pl00 GPU上训练仅十二小时后，即可达到翻译质量的新水平。

## 模型架构

大多数具有竞争力的神经序列转换模型都具有编码器-解码器结构5,2,35]。在这里，编码器将符号表示的输入序列(c1,,xn)映射到连续表示的序列z=(1,,n)。给定五，解码器随后逐个生成符号输出序列(1，，)。在每一步，模型都是自回归的[10]，在生成下一个符号时，将先前生成的符号作为额外的输入。

![2025-09-05_18-46.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/325875f2-d38d-4ecc-805d-9ce497f24662.jpeg)



整个流程可以概括为：

1. **Inputs** (源序列) → 编码器 → 生成**编码器的隐藏表示**。
2. **Outputs** (目标序列，右移) → 解码器 → 生成**解码器的隐藏表示**。
3. 解码器利用其隐藏表示和编码器的隐藏表示，通过注意力机制，预测下一个词。
4. 解码器的最终输出 → **Linear + Softmax** → **Output Probabilities**。
5. 将 `Output Probabilities` 与真实的下一个词进行比较，优化模型。

## 注意力

![2025-09-05_18-50.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/41be4764-92f3-4137-b9f6-746156961bf6.jpeg)

* **核心思想**： 注意力函数的本质是\*\*查询（Query）- 键（Key）- 值（Value）\*\*的映射。它不是一个黑盒，而是一个明确的计算过程：
  * 你有一组**值（Values）**，它们是你想要获取的“信息”。
  * 你有一个**查询（Query）**，它代表了你当前“想要知道什么”。
  * 你有一组**键（Keys）**，它们是每个“值”对应的“标签”或“索引”。
  * 注意力函数会计算查询（Query）与每一个键（Key）的“相似度”或“匹配度”。
  * 这些相似度被归一化成**权重（Weights）**。
  * 最终的输出是所有“值（Values）”的**加权和**，权重就是刚才计算出来的相似度。

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

* **公式化表达**： 论文将其描述为：

  > 输出（Output） = 以权重为系数，对值（Values）进行的加权和。 其中，分配给每个值的权重是通过查询（Query）与对应键（Key）的兼容性函数（compatibility function）计算得出的。
  >

  这个通用框架非常灵活，不同的“兼容性函数”就定义了不同类型的注意力。


### 点积注意力 (Dot-Product Attention)

这是Transformer中使用的核心注意力机制。在论文的3.2.1节，作者提出了他们具体实现的注意力，称为**缩放点积注意力（Scaled Dot-Product Attention）**。

* **为什么选择点积？** 论文提到了两种最常见的注意力函数：**加性注意力（Additive Attention）**和**点积注意力（Dot-Product Attention）**。

  * **加性注意力**：使用一个小型的前馈神经网络（带一个隐藏层）来计算Query和Key的兼容性。计算量相对较大。
  * **点积注意力**：直接计算Query和Key的点积（向量内积）作为兼容性分数。计算速度极快，并且可以利用高度优化的矩阵乘法库（如BLAS）来实现，非常高效。

  因此，在计算效率上，点积注意力远胜于加性注意力。
* **问题：点积的缩放 (The Scaling Problem)** 当Key向量的维度 `dk` 很大时，点积的结果会变得非常大。这会导致 `softmax` 函数的输入值过大，使其进入梯度极小的饱和区（如下图所示）。当梯度接近于零时，反向传播就无法有效地更新模型参数，导致训练困难。

*当输入值很大时，softmax的输出会趋近于0或1，其导数（梯度）会非常小。*

* **解决方案：缩放 (Scaling)** 为了解决这个问题，作者引入了一个关键的改进：**将点积的结果除以 `√dk`**。
  * **原理**：假设Query和Key向量的每个分量都是均值为0、方差为1的独立随机变量，那么它们的点积的方差就等于 `dk`。通过除以 `√dk`，可以将点积结果的方差重新稳定在1左右，从而防止其值过大，避免了softmax的饱和问题。

**最终公式**： 给定查询矩阵 `Q`（维度 `n x dk`）、键矩阵 `K`（维度 `m x dk`）和值矩阵 `V`（维度 `m x dv`），缩放点积注意力的计算公式为：

`Attention(Q, K, V) = softmax(QKᵀ / √dk) * V`

* `QKᵀ`：计算所有Query和Key之间的点积，得到一个 `n x m` 的分数矩阵。
* `/ √dk`：对分数矩阵进行缩放。
* `softmax(...)`：在`K`和`V`的序列长度维度（即`m`）上进行softmax归一化，得到注意力权重矩阵。每一行代表一个Query对所有Key的注意力分布。
* `* V`：用注意力权重矩阵对值矩阵 `V` 进行加权求和，得到最终的输出，其维度为 `n x dv`。

这个公式是Transformer中所有注意力层（自注意力、编码器-解码器注意力）的数学核心

### 多头注意力 (Multi-Head Attention)

这是Transformer模型的另一个创新点，旨在让模型能够从不同的“子空间”或“表示角度”来关注输入信息。

* **为什么需要多头？** 如果只使用一个单一的注意力头，那么所有Query、Key、Value的变换都共享同一套参数。这相当于强迫模型用一种固定的方式去理解所有依赖关系，可能会限制模型的表达能力。例如，一个头可能同时关注了语法结构和语义信息，导致信息混杂。
* **工作原理**： “多头”意味着模型并行地运行**多个**独立的注意力函数（即“头”），然后将它们的结果合并起来。

  * **线性投影**：首先，原始的Query、Key、Value矩阵会通过 `h` 组不同的线性变换（即不同的权重矩阵 `W_Q`, `W_K`, `W_V`）被投影到 `h` 个不同的低维子空间。
    * 每个头的Query、Key维度为 `dk`，Value维度为 `dv`。
    * 通常设置 `dk = dv = d_model / h`，以保证总计算量与单头注意力相当。
  * **并行计算**：在每个投影后的子空间里，独立地应用缩放点积注意力函数。
    * `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
  * **拼接与投影**：将 `h` 个头的输出（每个维度为 `dv`）\*\*拼接（Concatenate）\*\*在一起，形成一个维度为 `h * dv` 的向量。
  * 最后，再通过一个线性变换 `W_O`，将拼接后的向量投影回原始的 `d_model` 维度。
* **最终公式**： `MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O` 其中 `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
* **优势**：

  * **多样性**：不同的头可以学习到不同的关注模式。例如，有的头可能专注于语法结构（如主谓宾），有的头可能专注于指代关系（如代词和先行词），有的头可能关注语义相似性。
  * **鲁棒性**：即使某个头的注意力失效，其他头仍然可以提供有效信息。
  * **表达能力增强**：相当于模型拥有了多个“专家”，每个专家负责一个特定的方面，共同协作完成任务。

  论文中的实验（Table 3, row A）也证明了，使用多个头（8个）比使用单个头性能更好。

其中：


| 符号         | 含义                         | 如何确定             | 论文中的取值(BASE MODEL) |
| ------------ | ---------------------------- | -------------------- | ------------------------ |
| dmodel       | 模型的主干维度               | 模型的超参数         | 512                      |
| b            | 注意力头的数量               | 模型的超参数         | 8                        |
| dk           | 每个头中 Key 和 Query 的维度 | dk= d_model /h       | 64                       |
| dv           | 每个头中 Value 的维度        | dv=d model /h        | 64                       |
| Q,K,V (单头) | 单个注意力头的矩阵维度       | oatchsze,s或(asize e | (batchsiz,sg leng 1      |

### Q, K, V 的最终维度

在多头注意力的具体实现中，我们通常讨论的是**单个头**的 `Q`, `K`, `V` 矩阵的维度。

* **输入**：假设我们有一个批次的输入序列，其形状为 `(batch_size, sequence_length, d_model)`。
* **线性投影**：这个输入会分别乘以三个不同的权重矩阵 `W^Q`, `W^K`, `W^V`。
  * `W^Q` 的维度是 `(d_model, d_k)`。
  * `W^K` 的维度是 `(d_model, d_k)`。
  * `W^V` 的维度是 `(d_model, d_v)`。
* **输出维度**：
  * 经过线性投影后，对于**单个注意力头**，`Q`, `K`, `V` 矩阵的维度都是：
    * `(batch_size, sequence_length, d_k)` 对于 `Q` 和 `K`
    * `(batch_size, sequence_length, d_v)` 对于 `V`
  * 在论文的Base模型中，这具体为 `(batch_size, sequence_length, 64)`。

**注意**：在实际的代码实现中，为了效率，通常会一次性计算所有 `h` 个头。因此，权重矩阵的维度会是 `(d_model, h * d_k)`，投影后的 `Q`, `K`, `V` 矩阵的总维度为 `(batch_size, sequence_length, h * d_k)`，然后通过 `reshape` 或 `split` 操作将它们分到 `h` 个头上。


### 核心维度：`d_model`

这是整个Transformer模型的“主干”维度，也称为**模型维度（Model Dimension）**。

* **定义**：`d_model` 是模型中所有向量的标准大小。这包括：
  * 词嵌入（Word Embeddings）和位置编码（Positional Encodings）的维度。
  * 编码器和解码器每一层的输入和输出的维度。
  * 残差连接和层归一化操作的维度。
* **论文中的取值**：在论文的“Base”模型中，`d_model = 512`。

---

### 多头注意力的拆分：`d_k`, `d_v` 和 `h`

为了实现多头注意力，模型会将 `d_model` 维度的空间**线性投影**到 `h` 个不同的、更低维度的子空间中，每个子空间对应一个“头”（head）。

* `h` (Number of Heads)：注意力头的数量。
  * 论文中的取值：`h = 8`（对于Base模型）。
* **`d_k` (Key/Query Dimension)**：每个头中键（Key）**和**查询（Query）向量的维度。
* **`d_v` (Value Dimension)**：每个头中值（Value）向量的维度。

论文在 **3.2.2 节 "Multi-Head Attention"** 中明确指出：

> "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use **dk= dv= dmodel/h= 64**."

这意味着：

* **计算公式**：
  * `d_k = d_model / h`
  * `d_v = d_model / h`
* **论文中的取值**：
  * `d_k = 512 / 8 = 64`
  * `d_v = 512 / 8 = 64`

通过将 `d_model` 等分为 `h` 份，每个头的计算复杂度大大降低，从而保证了多头注意力的总计算量与单头注意力相当。

$$
\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_{1},...,\mathrm{head}_{\mathrm{h}})W^{O}\\\mathrm{where~head}_{\mathrm{i}}=\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})
$$


### 自注意力在模型中的三种应用 (Applications in the Model)

论文在3.2.3节详细说明了多头注意力如何在Transformer的编码器和解码器中被具体应用。

1. **编码器中的自注意力 (Encoder Self-Attention)**：
   * **Query, Key, Value来源**：三者都来自**同一个地方**——即编码器前一层的输出。
   * **作用**：允许输入序列中的**每一个位置**都能关注到序列中的**所有其他位置**。这使得模型能够建立单词之间的全局依赖关系，例如，理解一个代词指代的是哪个名词。
2. **解码器中的自注意力 (Decoder Self-Attention)**：
   * **特殊限制**：为了保证解码过程的**自回归（Auto-regressive）**特性（即在生成第 `i` 个词时，不能看到第 `i` 个及之后的词），这里引入了**掩码（Masking）**。
   * **实现方式**：在计算 `softmax(QKᵀ / √dk)` 之前，将所有“非法连接”（即未来位置的Key）所对应的分数设置为一个非常大的负数（如 `-∞`）。经过 `softmax` 后，这些位置的权重会变为0。
   * **作用**：确保在预测位置 `i` 的输出时，模型只能依赖于位置 `1` 到 `i-1` 的已知输出。
3. **编码器-解码器注意力 (Encoder-Decoder Attention)**：
   * **Query来源**：来自**解码器**的前一层输出。
   * **Key和Value来源**：来自**编码器**的最终输出。
   * **作用**：这是连接编码器和解码器的桥梁。它允许解码器在生成目标语言的每一个词时，都能“关注”到源语言输入序列中所有相关的部分。例如，在翻译“making”这个词时，模型可以关注到源句中的“registration or voting process”。

## 位置前馈网络

位置前馈网络（Position-wise Feed-Forward Networks, FFN）是Transformer架构中与自注意力机制（Self-Attention）同等重要的核心组件。它被设计用来补充和增强自注意力层的功能，解决其固有的局限性。

### 1. 引入非线性变换，增强模型表达能力

这是最根本的原因。

* **自注意力的局限性**：自注意力机制的核心计算是**线性变换**的组合。具体来说，它通过矩阵乘法（`QKᵀ`）和加权求和（`softmax(...) * V`）来计算输出。虽然 `softmax` 函数本身是非线性的，但整个注意力机制对输入 `Q, K, V` 的处理在本质上是线性的。
* **问题**：如果模型中只有线性操作，无论堆叠多少层，整个网络的表达能力等价于一个单层的线性变换，无法学习复杂的非线性关系。
* **FFN的解决方案**：位置前馈网络是一个简单的两层全连接网络，中间包含一个**非线性激活函数**（在论文中是 ReLU）。其公式为： `FFN(x) = max(0, xW₁ + b₁)W₂ + b₂` 这个 ReLU 激活函数 `max(0, ...)` 引入了强大的非线性，使得模型能够学习和表示输入数据中复杂的、非线性的模式。

---

### 2. 进行独立的位置变换（Position-wise Transformation）

FFN的“位置前馈”特性使其与自注意力机制形成完美互补。

* **自注意力的作用**：建立**序列内部**不同位置之间的依赖关系。它关注的是“全局”信息，即一个词与序列中所有其他词的关系。
* **FFN的作用**：对**序列中每个位置**的表示进行独立的、更深层次的变换。它关注的是“局部”信息，即如何更丰富地表示单个词或位置。
* **工作方式**：FFN被应用在序列的每一个位置上，且对每个位置使用**相同的参数**（即相同的权重矩阵 `W₁`, `W₂` 和偏置 `b₁`, `b₂`），但计算是独立进行的。这意味着位置 `i` 的输出只依赖于位置 `i` 的输入，不直接依赖于其他位置。这与自注意力的全局交互形成了鲜明对比。

这种设计使得模型的每一层都包含两个关键步骤：

1. **信息聚合**：通过自注意力，从整个序列中收集相关信息，更新每个位置的表示。
2. **信息处理**：通过FFN，对聚合后的信息进行非线性加工和特征提取，进一步提炼每个位置的语义。

---

### 3. 提供额外的模型容量和学习能力

FFN为模型增加了大量的可学习参数，从而提升了模型的整体容量。

* **参数量**：在论文的“Base”模型中，`d_model = 512`，`d_ff = 2048`。这意味着FFN层的参数量为：
  * 第一层权重：`512 x 2048`
  * 第二层权重：`2048 x 512`
  * 总计约 `512 * 2048 * 2 ≈ 2.1M` 个参数。 这与多头注意力层的参数量相当，甚至更多，是模型学习能力的重要组成部分。
* **功能类比**：论文在3.3节中提到，FFN“可以被看作是两个卷积核大小为1的卷积（two convolutions with kernel size 1）”。这强调了它是在每个位置上进行的独立特征变换，类似于图像处理中在每个像素上应用的1x1卷积。

---

### 4. 与残差连接和层归一化协同工作

FFN是Transformer残差网络结构中的关键一环。

* **残差连接**：FFN的输出会通过一个残差连接，与它的输入相加：`Output = LayerNorm(x + FFN(x))`。
* **作用**：这使得信息可以绕过FFN层直接传递，解决了深层网络中的梯度消失问题，让模型可以稳定地训练多达6层（或更多）的堆叠结构。

### 总结

位置前馈网络（FFN）在Transformer中的作用可以概括为：

* **它是自注意力机制的“搭档”**：自注意力负责**交互**（让每个词看到其他所有词），而FFN负责**处理**（对每个词的表示进行深度的非线性变换）。
* **它是非线性的“引擎”**：为整个由线性操作主导的注意力机制注入了必要的非线性，极大地增强了模型的表达能力。
* **它是模型容量的“基石”**：通过其大量的参数，为模型学习复杂语言模式提供了空间。

## 嵌入和Softmax

在这一步中作者在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，在嵌入层中，将这些权重乘以一个系数

### 1. 背景：权重共享 (Weight Sharing)

在论文中，作者使用了一个技巧来减少模型参数并提升性能：**共享输入/输出嵌入层和预Softmax层的权重矩阵**。

* **输入嵌入 (Input Embedding)**：将输入的 token ID 映射为一个 `d_model` 维的向量。其权重矩阵为 `W_e`，维度 `(vocab_size, d_model)`。
* **预Softmax线性变换 (Pre-softmax Linear Transformation)**：在解码器的最后，将隐藏状态映射回词汇表大小，以便计算下一个词的概率。其权重矩阵通常为 `W_o`，维度 `(d_model, vocab_size)`。
* **权重共享**：作者让 `W_o = W_e^T`（即 `W_o` 是 `W_e` 的转置）。这是一个常见的优化技巧。

---

### 2. 问题：方差不匹配 (Variance Mismatch)

当我们将一个 token ID 通过嵌入层 `W_e` 映射为嵌入向量 `e` 时，这个向量的每个元素（分量）可以看作是 `W_e` 的某一行与一个 one-hot 向量的点积。

* **嵌入向量的方差**：如果 `W_e` 的权重是随机初始化的（例如，均值为0，方差为 `1/d_model` 的正态分布），那么计算出的嵌入向量 `e` 的**方差**大约为 **1**。
* **位置编码的方差**：论文中使用的正弦/余弦位置编码（sinusoidal positional encoding）的值域在 `[-1, 1]` 之间，因此其方差远小于1（大约为 `1/3`）。

**核心问题**：如果直接将嵌入向量 `e` 和位置编码 `PE` 相加： `e + PE`

由于 `e` 的方差（约1）远大于 `PE` 的方差（约1/3），位置编码的信号就会被强大的嵌入向量“淹没”，导致位置信息在后续的计算中变得微不足道，这与引入位置编码的初衷相违背。

---

### 3. 解决方案：乘以 `√d_model`

为了解决这个问题，作者在**使用嵌入层权重 `W_e` 之前**，先将其乘以 `√d_model`。

* **具体操作**：

  * 在计算嵌入向量时，不是直接计算 `x * W_e`，而是计算 `x * (W_e * √d_model)`。
  * 这相当于将 `W_e` 的初始化方差从 `1/d_model` 提高到了 `1`。
  * 因此，计算出的嵌入向量 `e` 的方差就从约1**提高到了约 `d_model`**。
* **为什么这是解决方案？** 这个操作看似让问题更严重了（方差更大了），但它与后续的\*\*缩放点积注意力（Scaled Dot-Product Attention）\*\*中的缩放因子 `1/√dk` 是相互配合的。
  回顾注意力公式： `Attention(Q, K, V) = softmax(QKᵀ / √dk) * V`
  在编码器/解码器的**第一层**，查询（Q）和键（K）通常来自于 `Embedding + Positional Encoding`。

  * **Q 和 K 的方差**：由于 `Q` 和 `K` 都包含了被放大了的嵌入向量，它们的方差都很大（约 `d_model`）。
  * **QKᵀ 的方差**：两个大方差向量的点积，其方差会变得极其巨大（约 `d_model²`）。
  * **QKᵀ / √dk 的方差**：注意，这里的 `dk` 等于 `d_model`。因此，除以 `√d_model` 后，`QKᵀ / √d_model` 的方差被重新稳定在了 **`d_model`**。

  这个 `d_model` 的方差虽然仍然不小，但它是一个**可控且稳定**的值。更重要的是，这个操作确保了**嵌入向量和位置编码在数值尺度上是可比的**。
  通过将嵌入向量的方差放大，使得 `e` 和 `PE` 在相加时，`PE` 的贡献不会被完全忽略。尽管 `e` 的绝对值更大，但相对比例更合理，使得位置信息能够有效地融入到模型的表示中。

---

### 总结

乘以 `√d_model` 的系数是一个精巧的工程技巧，其主要目的和逻辑是：

1. **直接目的**：在嵌入层，通过放大权重，使得嵌入向量的方差增大。
2. **根本原因**：为了平衡**嵌入向量**和**位置编码**在数值上的贡献，防止位置信息被强大的嵌入信号淹没。
3. **协同作用**：这个操作与注意力机制中的 `1/√dk` 缩放因子协同工作，共同确保了模型输入和内部计算的数值稳定性。

简单来说，如果不乘以这个系数，位置编码的作用就会大打折扣；乘上这个系数后，虽然嵌入向量变大了，但整个系统的方差被设计得更加合理和稳定，从而保证了位置信息的有效性。

---



### 知乎大佬解释

作者：王四喜
链接：https://www.zhihu.com/question/415263284/answer/2010360549
来源：知乎

![2025-09-05_19-19.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/d5ce7e28-5d8c-4cce-b6fe-67899f2462cb.jpeg)


---

## 位置编码

### 为什么需要位置编码

这是理解位置编码的起点。

* **问题**：在自然语言中，词的顺序至关重要。句子 "The cat sat on the mat." 和 "The mat sat on the cat." 意思完全不同。RNN通过其循环结构（`h_t` 依赖于 `h_{t-1}`）天然地记住了顺序。CNN通过卷积核在序列上的滑动也隐含了位置信息。
* **Transformer的挑战**：Transformer的核心是\*\*自注意力（Self-Attention）\*\*机制。在计算注意力时，它会并行地考虑序列中所有词对之间的关系。例如，计算 "making" 的表示时，它会同时看到 "registration"、"voting"、"difficult" 等词。**这个过程是置换不变的（permutation-invariant）**，也就是说，如果你把输入序列的词打乱顺序，自注意力的输出在数学上是等价的。
* **结论**：为了弥补这一缺陷，必须显式地将**位置信息**注入到模型中。这就是位置编码的作用。

### 2. 位置编码的设计：正弦和余弦函数

论文提出了一种固定（非学习）的、基于正弦和余弦函数的位置编码方案。

* **公式**： 论文给出了以下公式来计算位置 `pos` 在维度 `i` 上的编码值：

  ```python
  PE(pos, 2i)   = sin(pos / 10000^(2i/d\_model))

  PE(pos, 2i+1) = cos(pos / 10000^(2i/d\_model))
  ```

  其中：

  * `pos` 是词在序列中的位置（0, 1, 2, ..., n-1）。
  * `i` 是编码向量的维度索引（0, 1, 2, ..., d\_model-1）。
  * `d_model` 是模型的维度（512）。
* **如何理解这个公式？**

  * **交替使用 sin 和 cos**：对于偶数维度（`2i`），使用 `sin` 函数；对于奇数维度（`2i+1`），使用 `cos` 函数。这确保了每个“维度对”都由一个正弦和一个余弦函数组成。
  * **不同的频率**：公式中的 `10000^(2i/d_model)` 是一个**波长（wavelength）参数**。关键在于，这个波长随着维度 `i` 的增加而**指数级增长**。
    * 当 `i=0` 时，波长 `λ ≈ 2π * 10000^0 = 2π`。
    * 当 `i` 增大时，`10000^(2i/d_model)` 变大，波长 `λ` 也随之变大。
    * 最大波长出现在 `i = d_model/2 - 1` 时，约为 `10000 * 2π`。
  * **几何级数的波长**：不同维度的正弦/余弦函数具有从 `2π` 到 `10000·2π` 的、形成**几何级数**的波长。这意味着模型可以捕捉到从非常精细的局部位置信息到非常粗糙的全局位置信息。
* **一个直观的例子**： 假设 `d_model = 4`，那么位置编码向量有4个维度：

  * `PE(pos, 0) = sin(pos / 10000^0) = sin(pos)`
  * `PE(pos, 1) = cos(pos / 10000^0) = cos(pos)`
  * `PE(pos, 2) = sin(pos / 10000^(2/4)) = sin(pos / 100)`
  * `PE(pos, 3) = cos(pos / 10000^(2/4)) = cos(pos / 100)`

  可以看到，前两个维度（`sin(pos)` 和 `cos(pos)`）变化非常快，对 `pos` 的微小变化很敏感。而后两个维度（`sin(pos/100)` 和 `cos(pos/100)`）变化非常慢，对 `pos` 的微小变化不敏感，更适合表示长距离的位置。

---

### *. 为什么选择这种正弦/余弦编码？

作者在论文中给出了两个主要原因：

1. **允许模型学习相对位置 (Learning Relative Positions)**： 这是最核心的优势。作者假设，使用这种特定的函数，模型可以**轻松地通过线性变换来学习相对位置**。
   * **数学原理**：对于任何固定的偏移量 `k`，`PE(pos+k)` 可以被表示为 `PE(pos)` 的一个**线性函数**。这得益于正弦和余弦函数的和角公式： `sin(a+b) = sin(a)cos(b) + cos(a)sin(b)``cos(a+b) = cos(a)cos(b) - sin(a)sin(b)`
   * **意义**：这意味着，如果模型需要知道“词B在词A之后3个位置”，它可以通过一个简单的线性层，从 `PE(pos_A)` 和 `PE(pos_B)` 的差值中推断出这个相对关系。这比让模型从零开始学习绝对位置之间的复杂非线性关系要容易得多。
2. **支持序列外推 (Extrapolation to Longer Sequences)**： 作者实验了**可学习的位置编码（learned positional embeddings）**，发现性能与正弦编码几乎相同。但他们最终选择了正弦编码，因为它是一个**固定的函数**。
   * **优势**：由于它是基于数学公式的，理论上它可以为**任意长的位置 `pos`** 生成编码，即使这个位置在训练数据中从未出现过。
   * **对比**：可学习的位置编码是一个查找表（lookup table），其大小是固定的（比如最多支持512个位置）。如果在推理时遇到超过512个词的句子，模型就无法处理。而正弦编码没有这个限制。

---

### 4. 如何应用位置编码？

位置编码的应用非常直接。

* **维度匹配**：位置编码向量 `PE(pos, i)` 的维度与词嵌入向量 `E(pos, i)` 完全相同，都是 `d_model`。
* **相加（Sum）**：在将输入送入编码器或解码器堆栈之前，将词嵌入向量和位置编码向量**逐元素相加**。 `Input = E(pos, i) + PE(pos, i)`
* **目的**：这样，模型的输入就同时包含了词的语义信息（来自词嵌入）和词的位置信息（来自位置编码）。后续的所有层（自注意力、FFN等）都可以同时利用这两种信息。

---

### 5. 与可学习位置编码的对比

如论文6.2节（Table 3, row E）所述，作者也尝试了可学习的位置编码（即一个可训练的嵌入层，为每个位置 `pos` 学习一个 `d_model` 维的向量）。

* **结果**：两种方法在机器翻译任务上取得了**几乎相同的结果**。
* **选择**：作者最终选择了正弦/余弦编码，主要是基于其**外推能力**的理论优势。

---

### 总结

位置编码是Transformer模型感知序列顺序的“生命线”。其设计精巧，主要特点如下：

* **必要性**：弥补了自注意力机制丢失顺序信息的缺陷。
* **实现方式**：使用正弦和余弦函数生成固定的位置编码，并与词嵌入相加。
* **核心优势**：
  1. **学习相对位置**：通过线性变换，模型能轻松地学习词与词之间的相对距离。
  2. **支持外推**：固定的数学公式使其可以处理任意长度的序列。
* **有效性**：虽然可学习编码效果相当，但正弦/余弦编码因其优雅的数学性质和理论上的鲁棒性而被选为标准方案。

### 知乎大佬解释

Transformer学习笔记一：Positional Encoding（位置编码） - 猛猿的文章 - 知乎
https://zhuanlan.zhihu.com/p/454482273

![2025-09-05_19-31.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/59ed555d-26f1-4816-a6fa-0b6644972510.jpeg)

## 自注意力的优势

![2025-09-05_19-35.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/40a3c2c1-33fb-4d5d-8197-b49507d2ec29.jpeg)

### 1. 每层复杂度 (Complexity per Layer)

这指的是计算一层网络所有输出所需的浮点运算次数（FLOPs）。

* 自注意力 (Self-Attention): `O(n² · d)`
  * **计算过程**：自注意力的核心是计算查询（Query）和键（Key）之间的点积。假设序列长度为 `n`，每个向量的维度为 `d`。
    * 计算一个查询向量 `q` 与所有 `n` 个键向量 `k` 的点积，需要 `n * d` 次乘法和 `n * (d-1)` 次加法，近似为 `O(n*d)`。
    * 序列中有 `n` 个查询，所以总计算量为 `n * O(n*d) = O(n²*d)`。
  * **结论**：复杂度主要由 `n²` 项主导，因为需要计算所有位置之间的注意力分数。
* 循环 (Recurrent): `O(n · d²)`
  * **计算过程**：RNN在每个时间步 `t` 的计算通常涉及矩阵乘法，例如 `h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)`。
    * 两个 `d x d` 矩阵相乘的复杂度是 `O(d²)`。
    * 这个操作需要对序列中的 `n` 个位置都执行一次。
  * **结论**：总复杂度为 `n * O(d²) = O(n·d²)`。
* 卷积 (Convolutional): `O(k · n · d²)`
  * **计算过程**：对于一个一维卷积层，其核大小为 `k`。
    * 在一个位置进行卷积操作，需要对 `k` 个输入向量（每个维度为 `d`）与一个 `k x d x d` 的卷积核进行计算，复杂度约为 `O(k·d²)`。
    * 这个操作需要滑动 `n` 次（忽略填充和步长的影响）。
  * **结论**：总复杂度为 `n * O(k·d²) = O(k·n·d²)`。

---

### 2. 最小顺序操作数 (Minimum Number of Sequential Operations)

这衡量的是**并行化程度**。它表示在计算一个序列的输出时，理论上最少需要多少个必须按顺序执行的操作。

* 自注意力 (Self-Attention): `O(1)`

  * **解释**：自注意力的所有计算（计算 Q, K, V 矩阵、点积、Softmax、加权求和）都可以**完全并行**地对整个序列进行。没有操作是依赖于前一个时间步的输出的。因此，理论上，所有操作可以在一个“步骤”内完成，即 `O(1)`。
* 循环 (Recurrent): `O(n)`

  * **解释**：RNN 的本质是顺序的。要计算第 `t` 个隐藏状态 `h_t`，**必须先计算出**`h_{t-1}`。因此，计算一个长度为 `n` 的序列，至少需要 `n` 个顺序步骤。
* 卷积 (Convolutional): `O(1)`

  * **解释**：卷积操作本身是并行的。计算输出序列中每一个位置的值，只依赖于输入序列的一个局部窗口，与其他位置的计算完全独立。因此，所有 `n` 个卷积操作可以同时进行，最小顺序操作数为 `O(1)`。

---

### 3. 最大路径长度 (Maximum Path Length)

这衡量的是信息在网络中传播的效率，特别是对于学习**长距离依赖**的能力。它表示从输入序列中的任意一个位置到输出序列中任意一个位置，信号需要经过的最短路径上的操作数量。

* 自注意力 (Self-Attention): `O(1)`
  * **解释**：这是自注意力最强大的优势之一。在自注意力层中，序列中的**任意两个位置**都可以通过一个**直接的注意力连接**进行交互。无论两个词相距多远，它们之间的依赖路径长度都是**1**（通过一次注意力计算）。因此，最大路径长度是常数 `O(1)`。
* 循环 (Recurrent): `O(n)`
  * **解释**：在RNN中，位置 `1` 的信息要传递到位置 `n`，必须依次经过 `h_1 -> h_2 -> h_3 -> ... -> h_n`，总共需要经过 `n-1` 次RNN单元的计算。因此，路径长度是 `O(n)`。
* 卷积 (Convolutional): `O(log_k(n))`
  * **解释**：一个卷积核的“感受野”是有限的（大小为 `k`）。要让位置 `1` 的信息影响到位置 `n`，需要堆叠多层卷积。
    * 经过一层卷积，感受野大小为 `k`。
    * 经过两层卷积，感受野大小为 `k + (k-1) = 2k-1`（假设步长为1）。
    * 经过 `L` 层卷积，感受野大小约为 `L*(k-1) + 1`。
    * 要覆盖整个长度 `n` 的序列，需要 `L ≈ n/k` 层（如果使用普通卷积）。
  * **论文中的 `O(log_k(n))`**：这个更优的 `O(log_k(n))` 路径长度通常是通过\*\*空洞卷积（Dilated Convolution）\*\*实现的，如ByteNet和WaveNet。空洞卷积通过在卷积核中引入“空洞”来指数级地扩大感受野。例如，第一层空洞为1，第二层空洞为2，第三层空洞为4... 这样，经过 `L` 层后，感受野可以达到 `O(k^L)`。因此，要覆盖长度 `n`，只需要 `L = O(log_k(n))` 层。


### 大佬解释

https://tobiaslee.top/2018/12/13/Start-from-Transformer/

![2025-09-05_19-40.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/1e1bdeee-816f-4b33-a28b-727179b35fab.jpeg)


Transformer/CNN/RNN的对比（时间复杂度，序列操作数，最大路径长度） - Gordon Lee的文章 - 知乎
https://zhuanlan.zhihu.com/p/264749298
