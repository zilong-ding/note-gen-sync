# AttentionIsAllYouNeed论文解读

## 背景

RNN,LSTM,GRU是当时比较序列建模，语言建模和机器翻译中最先进的方法，但是这三种方法都是属于串行结构。串行结构问题在于并行效率低和层数过多是会出现信息丢失的现象。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许在忽略输入或输出序列中依赖关系距离的情况下进行建模2,19]。然而，在除少数情况27]之外的所有情况下，此类注意力机制都与循环网络结合使用。

在这个工作中，我们提出了Transformer,一种摒弃递归并完全依赖注意力机制来在输入和输出之间建立全局依赖关系的模型架构。Transformer允许实现更高级别的并行化，在八个Pl00 GPU上训练仅十二小时后，即可达到翻译质量的新水平。

## 模型架构

大多数具有竞争力的神经序列转换模型都具有编码器-解码器结构5,2,35]。在这里，编码器将符号表示的输入序列(c1,,xn)映射到连续表示的序列z=(1,,n)。给定五，解码器随后逐个生成符号输出序列(1，，)。在每一步，模型都是自回归的[10]，在生成下一个符号时，将先前生成的符号作为额外的输入。

![2025-09-05_18-46.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/325875f2-d38d-4ecc-805d-9ce497f24662.jpeg)



整个流程可以概括为：

1. **Inputs** (源序列) → 编码器 → 生成**编码器的隐藏表示**。
2. **Outputs** (目标序列，右移) → 解码器 → 生成**解码器的隐藏表示**。
3. 解码器利用其隐藏表示和编码器的隐藏表示，通过注意力机制，预测下一个词。
4. 解码器的最终输出 → **Linear + Softmax** → **Output Probabilities**。
5. 将 `Output Probabilities` 与真实的下一个词进行比较，优化模型。

## 注意力

![2025-09-05_18-50.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/41be4764-92f3-4137-b9f6-746156961bf6.jpeg)

* **核心思想**： 注意力函数的本质是\*\*查询（Query）- 键（Key）- 值（Value）\*\*的映射。它不是一个黑盒，而是一个明确的计算过程：
  * 你有一组**值（Values）**，它们是你想要获取的“信息”。
  * 你有一个**查询（Query）**，它代表了你当前“想要知道什么”。
  * 你有一组**键（Keys）**，它们是每个“值”对应的“标签”或“索引”。
  * 注意力函数会计算查询（Query）与每一个键（Key）的“相似度”或“匹配度”。
  * 这些相似度被归一化成**权重（Weights）**。
  * 最终的输出是所有“值（Values）”的**加权和**，权重就是刚才计算出来的相似度。

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

* **公式化表达**： 论文将其描述为：

  > 输出（Output） = 以权重为系数，对值（Values）进行的加权和。 其中，分配给每个值的权重是通过查询（Query）与对应键（Key）的兼容性函数（compatibility function）计算得出的。
  >

  这个通用框架非常灵活，不同的“兼容性函数”就定义了不同类型的注意力。


### 点积注意力 (Dot-Product Attention)

这是Transformer中使用的核心注意力机制。在论文的3.2.1节，作者提出了他们具体实现的注意力，称为**缩放点积注意力（Scaled Dot-Product Attention）**。

* **为什么选择点积？** 论文提到了两种最常见的注意力函数：**加性注意力（Additive Attention）**和**点积注意力（Dot-Product Attention）**。

  * **加性注意力**：使用一个小型的前馈神经网络（带一个隐藏层）来计算Query和Key的兼容性。计算量相对较大。
  * **点积注意力**：直接计算Query和Key的点积（向量内积）作为兼容性分数。计算速度极快，并且可以利用高度优化的矩阵乘法库（如BLAS）来实现，非常高效。

  因此，在计算效率上，点积注意力远胜于加性注意力。
* **问题：点积的缩放 (The Scaling Problem)** 当Key向量的维度 `dk` 很大时，点积的结果会变得非常大。这会导致 `softmax` 函数的输入值过大，使其进入梯度极小的饱和区（如下图所示）。当梯度接近于零时，反向传播就无法有效地更新模型参数，导致训练困难。

*当输入值很大时，softmax的输出会趋近于0或1，其导数（梯度）会非常小。*

* **解决方案：缩放 (Scaling)** 为了解决这个问题，作者引入了一个关键的改进：**将点积的结果除以 `√dk`**。
  * **原理**：假设Query和Key向量的每个分量都是均值为0、方差为1的独立随机变量，那么它们的点积的方差就等于 `dk`。通过除以 `√dk`，可以将点积结果的方差重新稳定在1左右，从而防止其值过大，避免了softmax的饱和问题。

**最终公式**： 给定查询矩阵 `Q`（维度 `n x dk`）、键矩阵 `K`（维度 `m x dk`）和值矩阵 `V`（维度 `m x dv`），缩放点积注意力的计算公式为：

`Attention(Q, K, V) = softmax(QKᵀ / √dk) * V`

* `QKᵀ`：计算所有Query和Key之间的点积，得到一个 `n x m` 的分数矩阵。
* `/ √dk`：对分数矩阵进行缩放。
* `softmax(...)`：在`K`和`V`的序列长度维度（即`m`）上进行softmax归一化，得到注意力权重矩阵。每一行代表一个Query对所有Key的注意力分布。
* `* V`：用注意力权重矩阵对值矩阵 `V` 进行加权求和，得到最终的输出，其维度为 `n x dv`。

这个公式是Transformer中所有注意力层（自注意力、编码器-解码器注意力）的数学核心

### 多头注意力 (Multi-Head Attention)

这是Transformer模型的另一个创新点，旨在让模型能够从不同的“子空间”或“表示角度”来关注输入信息。

* **为什么需要多头？** 如果只使用一个单一的注意力头，那么所有Query、Key、Value的变换都共享同一套参数。这相当于强迫模型用一种固定的方式去理解所有依赖关系，可能会限制模型的表达能力。例如，一个头可能同时关注了语法结构和语义信息，导致信息混杂。
* **工作原理**： “多头”意味着模型并行地运行**多个**独立的注意力函数（即“头”），然后将它们的结果合并起来。

  * **线性投影**：首先，原始的Query、Key、Value矩阵会通过 `h` 组不同的线性变换（即不同的权重矩阵 `W_Q`, `W_K`, `W_V`）被投影到 `h` 个不同的低维子空间。
    * 每个头的Query、Key维度为 `dk`，Value维度为 `dv`。
    * 通常设置 `dk = dv = d_model / h`，以保证总计算量与单头注意力相当。
  * **并行计算**：在每个投影后的子空间里，独立地应用缩放点积注意力函数。
    * `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
  * **拼接与投影**：将 `h` 个头的输出（每个维度为 `dv`）\*\*拼接（Concatenate）\*\*在一起，形成一个维度为 `h * dv` 的向量。
  * 最后，再通过一个线性变换 `W_O`，将拼接后的向量投影回原始的 `d_model` 维度。
* **最终公式**： `MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O` 其中 `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
* **优势**：

  * **多样性**：不同的头可以学习到不同的关注模式。例如，有的头可能专注于语法结构（如主谓宾），有的头可能专注于指代关系（如代词和先行词），有的头可能关注语义相似性。
  * **鲁棒性**：即使某个头的注意力失效，其他头仍然可以提供有效信息。
  * **表达能力增强**：相当于模型拥有了多个“专家”，每个专家负责一个特定的方面，共同协作完成任务。

  论文中的实验（Table 3, row A）也证明了，使用多个头（8个）比使用单个头性能更好。

其中：


| 符号         | 含义                         | 如何确定             | 论文中的取值(BASE MODEL) |
| ------------ | ---------------------------- | -------------------- | ------------------------ |
| dmodel       | 模型的主干维度               | 模型的超参数         | 512                      |
| b            | 注意力头的数量               | 模型的超参数         | 8                        |
| dk           | 每个头中 Key 和 Query 的维度 | dk= d_model /h       | 64                       |
| dv           | 每个头中 Value 的维度        | dv=d model /h        | 64                       |
| Q,K,V (单头) | 单个注意力头的矩阵维度       | oatchsze,s或(asize e | (batchsiz,sg leng 1      |

### Q, K, V 的最终维度

在多头注意力的具体实现中，我们通常讨论的是**单个头**的 `Q`, `K`, `V` 矩阵的维度。

* **输入**：假设我们有一个批次的输入序列，其形状为 `(batch_size, sequence_length, d_model)`。
* **线性投影**：这个输入会分别乘以三个不同的权重矩阵 `W^Q`, `W^K`, `W^V`。
  * `W^Q` 的维度是 `(d_model, d_k)`。
  * `W^K` 的维度是 `(d_model, d_k)`。
  * `W^V` 的维度是 `(d_model, d_v)`。
* **输出维度**：
  * 经过线性投影后，对于**单个注意力头**，`Q`, `K`, `V` 矩阵的维度都是：
    * `(batch_size, sequence_length, d_k)` 对于 `Q` 和 `K`
    * `(batch_size, sequence_length, d_v)` 对于 `V`
  * 在论文的Base模型中，这具体为 `(batch_size, sequence_length, 64)`。

**注意**：在实际的代码实现中，为了效率，通常会一次性计算所有 `h` 个头。因此，权重矩阵的维度会是 `(d_model, h * d_k)`，投影后的 `Q`, `K`, `V` 矩阵的总维度为 `(batch_size, sequence_length, h * d_k)`，然后通过 `reshape` 或 `split` 操作将它们分到 `h` 个头上。


### 核心维度：`d_model`

这是整个Transformer模型的“主干”维度，也称为**模型维度（Model Dimension）**。

* **定义**：`d_model` 是模型中所有向量的标准大小。这包括：
  * 词嵌入（Word Embeddings）和位置编码（Positional Encodings）的维度。
  * 编码器和解码器每一层的输入和输出的维度。
  * 残差连接和层归一化操作的维度。
* **论文中的取值**：在论文的“Base”模型中，`d_model = 512`。

---

### 多头注意力的拆分：`d_k`, `d_v` 和 `h`

为了实现多头注意力，模型会将 `d_model` 维度的空间**线性投影**到 `h` 个不同的、更低维度的子空间中，每个子空间对应一个“头”（head）。

* `h` (Number of Heads)：注意力头的数量。
  * 论文中的取值：`h = 8`（对于Base模型）。
* **`d_k` (Key/Query Dimension)**：每个头中键（Key）**和**查询（Query）向量的维度。
* **`d_v` (Value Dimension)**：每个头中值（Value）向量的维度。

论文在 **3.2.2 节 "Multi-Head Attention"** 中明确指出：

> "In this work we employ h= 8 parallel attention layers, or heads. For each of these we use **dk= dv= dmodel/h= 64**."

这意味着：

* **计算公式**：
  * `d_k = d_model / h`
  * `d_v = d_model / h`
* **论文中的取值**：
  * `d_k = 512 / 8 = 64`
  * `d_v = 512 / 8 = 64`

通过将 `d_model` 等分为 `h` 份，每个头的计算复杂度大大降低，从而保证了多头注意力的总计算量与单头注意力相当。

$$
\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_{1},...,\mathrm{head}_{\mathrm{h}})W^{O}\\\mathrm{where~head}_{\mathrm{i}}=\mathrm{Attention}(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})
$$


### 自注意力在模型中的三种应用 (Applications in the Model)

论文在3.2.3节详细说明了多头注意力如何在Transformer的编码器和解码器中被具体应用。

1. **编码器中的自注意力 (Encoder Self-Attention)**：
   * **Query, Key, Value来源**：三者都来自**同一个地方**——即编码器前一层的输出。
   * **作用**：允许输入序列中的**每一个位置**都能关注到序列中的**所有其他位置**。这使得模型能够建立单词之间的全局依赖关系，例如，理解一个代词指代的是哪个名词。
2. **解码器中的自注意力 (Decoder Self-Attention)**：
   * **特殊限制**：为了保证解码过程的**自回归（Auto-regressive）**特性（即在生成第 `i` 个词时，不能看到第 `i` 个及之后的词），这里引入了**掩码（Masking）**。
   * **实现方式**：在计算 `softmax(QKᵀ / √dk)` 之前，将所有“非法连接”（即未来位置的Key）所对应的分数设置为一个非常大的负数（如 `-∞`）。经过 `softmax` 后，这些位置的权重会变为0。
   * **作用**：确保在预测位置 `i` 的输出时，模型只能依赖于位置 `1` 到 `i-1` 的已知输出。
3. **编码器-解码器注意力 (Encoder-Decoder Attention)**：
   * **Query来源**：来自**解码器**的前一层输出。
   * **Key和Value来源**：来自**编码器**的最终输出。
   * **作用**：这是连接编码器和解码器的桥梁。它允许解码器在生成目标语言的每一个词时，都能“关注”到源语言输入序列中所有相关的部分。例如，在翻译“making”这个词时，模型可以关注到源句中的“registration or voting process”。

## 位置前馈网络

位置前馈网络（Position-wise Feed-Forward Networks, FFN）是Transformer架构中与自注意力机制（Self-Attention）同等重要的核心组件。它被设计用来补充和增强自注意力层的功能，解决其固有的局限性。

### 1. 引入非线性变换，增强模型表达能力

这是最根本的原因。

* **自注意力的局限性**：自注意力机制的核心计算是**线性变换**的组合。具体来说，它通过矩阵乘法（`QKᵀ`）和加权求和（`softmax(...) * V`）来计算输出。虽然 `softmax` 函数本身是非线性的，但整个注意力机制对输入 `Q, K, V` 的处理在本质上是线性的。
* **问题**：如果模型中只有线性操作，无论堆叠多少层，整个网络的表达能力等价于一个单层的线性变换，无法学习复杂的非线性关系。
* **FFN的解决方案**：位置前馈网络是一个简单的两层全连接网络，中间包含一个**非线性激活函数**（在论文中是 ReLU）。其公式为： `FFN(x) = max(0, xW₁ + b₁)W₂ + b₂` 这个 ReLU 激活函数 `max(0, ...)` 引入了强大的非线性，使得模型能够学习和表示输入数据中复杂的、非线性的模式。

---

### 2. 进行独立的位置变换（Position-wise Transformation）

FFN的“位置前馈”特性使其与自注意力机制形成完美互补。

* **自注意力的作用**：建立**序列内部**不同位置之间的依赖关系。它关注的是“全局”信息，即一个词与序列中所有其他词的关系。
* **FFN的作用**：对**序列中每个位置**的表示进行独立的、更深层次的变换。它关注的是“局部”信息，即如何更丰富地表示单个词或位置。
* **工作方式**：FFN被应用在序列的每一个位置上，且对每个位置使用**相同的参数**（即相同的权重矩阵 `W₁`, `W₂` 和偏置 `b₁`, `b₂`），但计算是独立进行的。这意味着位置 `i` 的输出只依赖于位置 `i` 的输入，不直接依赖于其他位置。这与自注意力的全局交互形成了鲜明对比。

这种设计使得模型的每一层都包含两个关键步骤：

1. **信息聚合**：通过自注意力，从整个序列中收集相关信息，更新每个位置的表示。
2. **信息处理**：通过FFN，对聚合后的信息进行非线性加工和特征提取，进一步提炼每个位置的语义。

---

### 3. 提供额外的模型容量和学习能力

FFN为模型增加了大量的可学习参数，从而提升了模型的整体容量。

* **参数量**：在论文的“Base”模型中，`d_model = 512`，`d_ff = 2048`。这意味着FFN层的参数量为：
  * 第一层权重：`512 x 2048`
  * 第二层权重：`2048 x 512`
  * 总计约 `512 * 2048 * 2 ≈ 2.1M` 个参数。 这与多头注意力层的参数量相当，甚至更多，是模型学习能力的重要组成部分。
* **功能类比**：论文在3.3节中提到，FFN“可以被看作是两个卷积核大小为1的卷积（two convolutions with kernel size 1）”。这强调了它是在每个位置上进行的独立特征变换，类似于图像处理中在每个像素上应用的1x1卷积。

---

### 4. 与残差连接和层归一化协同工作

FFN是Transformer残差网络结构中的关键一环。

* **残差连接**：FFN的输出会通过一个残差连接，与它的输入相加：`Output = LayerNorm(x + FFN(x))`。
* **作用**：这使得信息可以绕过FFN层直接传递，解决了深层网络中的梯度消失问题，让模型可以稳定地训练多达6层（或更多）的堆叠结构。

### 总结

位置前馈网络（FFN）在Transformer中的作用可以概括为：

* **它是自注意力机制的“搭档”**：自注意力负责**交互**（让每个词看到其他所有词），而FFN负责**处理**（对每个词的表示进行深度的非线性变换）。
* **它是非线性的“引擎”**：为整个由线性操作主导的注意力机制注入了必要的非线性，极大地增强了模型的表达能力。
* **它是模型容量的“基石”**：通过其大量的参数，为模型学习复杂语言模式提供了空间。

## 嵌入和Softmax

在这一步中作者在两个嵌入层和预softmax线性变换之间共享相同的权重矩阵，在嵌入层中，将这些权重乘以一个系数

### 1. 背景：权重共享 (Weight Sharing)

在论文中，作者使用了一个技巧来减少模型参数并提升性能：**共享输入/输出嵌入层和预Softmax层的权重矩阵**。

* **输入嵌入 (Input Embedding)**：将输入的 token ID 映射为一个 `d_model` 维的向量。其权重矩阵为 `W_e`，维度 `(vocab_size, d_model)`。
* **预Softmax线性变换 (Pre-softmax Linear Transformation)**：在解码器的最后，将隐藏状态映射回词汇表大小，以便计算下一个词的概率。其权重矩阵通常为 `W_o`，维度 `(d_model, vocab_size)`。
* **权重共享**：作者让 `W_o = W_e^T`（即 `W_o` 是 `W_e` 的转置）。这是一个常见的优化技巧。

---

### 2. 问题：方差不匹配 (Variance Mismatch)

当我们将一个 token ID 通过嵌入层 `W_e` 映射为嵌入向量 `e` 时，这个向量的每个元素（分量）可以看作是 `W_e` 的某一行与一个 one-hot 向量的点积。

* **嵌入向量的方差**：如果 `W_e` 的权重是随机初始化的（例如，均值为0，方差为 `1/d_model` 的正态分布），那么计算出的嵌入向量 `e` 的**方差**大约为 **1**。
* **位置编码的方差**：论文中使用的正弦/余弦位置编码（sinusoidal positional encoding）的值域在 `[-1, 1]` 之间，因此其方差远小于1（大约为 `1/3`）。

**核心问题**：如果直接将嵌入向量 `e` 和位置编码 `PE` 相加： `e + PE`

由于 `e` 的方差（约1）远大于 `PE` 的方差（约1/3），位置编码的信号就会被强大的嵌入向量“淹没”，导致位置信息在后续的计算中变得微不足道，这与引入位置编码的初衷相违背。

---

### 3. 解决方案：乘以 `√d_model`

为了解决这个问题，作者在**使用嵌入层权重 `W_e` 之前**，先将其乘以 `√d_model`。

* **具体操作**：

  * 在计算嵌入向量时，不是直接计算 `x * W_e`，而是计算 `x * (W_e * √d_model)`。
  * 这相当于将 `W_e` 的初始化方差从 `1/d_model` 提高到了 `1`。
  * 因此，计算出的嵌入向量 `e` 的方差就从约1**提高到了约 `d_model`**。
* **为什么这是解决方案？** 这个操作看似让问题更严重了（方差更大了），但它与后续的\*\*缩放点积注意力（Scaled Dot-Product Attention）\*\*中的缩放因子 `1/√dk` 是相互配合的。
  回顾注意力公式： `Attention(Q, K, V) = softmax(QKᵀ / √dk) * V`
  在编码器/解码器的**第一层**，查询（Q）和键（K）通常来自于 `Embedding + Positional Encoding`。

  * **Q 和 K 的方差**：由于 `Q` 和 `K` 都包含了被放大了的嵌入向量，它们的方差都很大（约 `d_model`）。
  * **QKᵀ 的方差**：两个大方差向量的点积，其方差会变得极其巨大（约 `d_model²`）。
  * **QKᵀ / √dk 的方差**：注意，这里的 `dk` 等于 `d_model`。因此，除以 `√d_model` 后，`QKᵀ / √d_model` 的方差被重新稳定在了 **`d_model`**。

  这个 `d_model` 的方差虽然仍然不小，但它是一个**可控且稳定**的值。更重要的是，这个操作确保了**嵌入向量和位置编码在数值尺度上是可比的**。
  通过将嵌入向量的方差放大，使得 `e` 和 `PE` 在相加时，`PE` 的贡献不会被完全忽略。尽管 `e` 的绝对值更大，但相对比例更合理，使得位置信息能够有效地融入到模型的表示中。

---

### 总结

乘以 `√d_model` 的系数是一个精巧的工程技巧，其主要目的和逻辑是：

1. **直接目的**：在嵌入层，通过放大权重，使得嵌入向量的方差增大。
2. **根本原因**：为了平衡**嵌入向量**和**位置编码**在数值上的贡献，防止位置信息被强大的嵌入信号淹没。
3. **协同作用**：这个操作与注意力机制中的 `1/√dk` 缩放因子协同工作，共同确保了模型输入和内部计算的数值稳定性。

简单来说，如果不乘以这个系数，位置编码的作用就会大打折扣；乘上这个系数后，虽然嵌入向量变大了，但整个系统的方差被设计得更加合理和稳定，从而保证了位置信息的有效性。

---



### 知乎回答

作者：王四喜
链接：https://www.zhihu.com/question/415263284/answer/2010360549
来源：知乎

![2025-09-05_19-19.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/d5ce7e28-5d8c-4cce-b6fe-67899f2462cb.jpeg)


---

## 位置编码

### 为什么需要位置编码
