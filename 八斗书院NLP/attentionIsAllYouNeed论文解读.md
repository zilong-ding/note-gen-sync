AttentionIsAllYouNeed论文解读

背景：

RNN,LSTM,GRU是当时比较序列建模，语言建模和机器翻译中最先进的方法，但是这三种方法都是属于串行结构。串行结构问题在于并行效率低和层数过多是会出现信息丢失的现象。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许在忽略输入或输出序列中依赖关系距离的情况下进行建模2,19]。然而，在除少数情况27]之外的所有情况下，此类注意力机制都与循环网络结合使用。

在这个工作中，我们提出了Transformer,一种摒弃递归并完全依赖注意力机制来在输入和输出之间建立全局依赖关系的模型架构。Transformer允许实现更高级别的并行化，在八个Pl00 GPU上训练仅十二小时后，即可达到翻译质量的新水平。
