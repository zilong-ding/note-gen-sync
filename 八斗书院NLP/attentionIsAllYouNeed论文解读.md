# AttentionIsAllYouNeed论文解读

## 背景

RNN,LSTM,GRU是当时比较序列建模，语言建模和机器翻译中最先进的方法，但是这三种方法都是属于串行结构。串行结构问题在于并行效率低和层数过多是会出现信息丢失的现象。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许在忽略输入或输出序列中依赖关系距离的情况下进行建模2,19]。然而，在除少数情况27]之外的所有情况下，此类注意力机制都与循环网络结合使用。

在这个工作中，我们提出了Transformer,一种摒弃递归并完全依赖注意力机制来在输入和输出之间建立全局依赖关系的模型架构。Transformer允许实现更高级别的并行化，在八个Pl00 GPU上训练仅十二小时后，即可达到翻译质量的新水平。

## 模型架构

大多数具有竞争力的神经序列转换模型都具有编码器-解码器结构5,2,35]。在这里，编码器将符号表示的输入序列(c1,,xn)映射到连续表示的序列z=(1,,n)。给定五，解码器随后逐个生成符号输出序列(1，，)。在每一步，模型都是自回归的[10]，在生成下一个符号时，将先前生成的符号作为额外的输入。

![2025-09-05_18-46.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/325875f2-d38d-4ecc-805d-9ce497f24662.jpeg)



整个流程可以概括为：

1. **Inputs** (源序列) → 编码器 → 生成**编码器的隐藏表示**。
2. **Outputs** (目标序列，右移) → 解码器 → 生成**解码器的隐藏表示**。
3. 解码器利用其隐藏表示和编码器的隐藏表示，通过注意力机制，预测下一个词。
4. 解码器的最终输出 → **Linear + Softmax** → **Output Probabilities**。
5. 将 `Output Probabilities` 与真实的下一个词进行比较，优化模型。

## 注意力

![2025-09-05_18-50.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/41be4764-92f3-4137-b9f6-746156961bf6.jpeg)

* **核心思想**： 注意力函数的本质是\*\*查询（Query）- 键（Key）- 值（Value）\*\*的映射。它不是一个黑盒，而是一个明确的计算过程：
  * 你有一组**值（Values）**，它们是你想要获取的“信息”。
  * 你有一个**查询（Query）**，它代表了你当前“想要知道什么”。
  * 你有一组**键（Keys）**，它们是每个“值”对应的“标签”或“索引”。
  * 注意力函数会计算查询（Query）与每一个键（Key）的“相似度”或“匹配度”。
  * 这些相似度被归一化成**权重（Weights）**。
  * 最终的输出是所有“值（Values）”的**加权和**，权重就是刚才计算出来的相似度。

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

* **公式化表达**： 论文将其描述为：

  > 输出（Output） = 以权重为系数，对值（Values）进行的加权和。 其中，分配给每个值的权重是通过查询（Query）与对应键（Key）的兼容性函数（compatibility function）计算得出的。
  >

  这个通用框架非常灵活，不同的“兼容性函数”就定义了不同类型的注意力。


### 点积注意力 (Dot-Product Attention)

这是Transformer中使用的核心注意力机制。在论文的3.2.1节，作者提出了他们具体实现的注意力，称为**缩放点积注意力（Scaled Dot-Product Attention）**。

* **为什么选择点积？** 论文提到了两种最常见的注意力函数：**加性注意力（Additive Attention）**和**点积注意力（Dot-Product Attention）**。

  * **加性注意力**：使用一个小型的前馈神经网络（带一个隐藏层）来计算Query和Key的兼容性。计算量相对较大。
  * **点积注意力**：直接计算Query和Key的点积（向量内积）作为兼容性分数。计算速度极快，并且可以利用高度优化的矩阵乘法库（如BLAS）来实现，非常高效。

  因此，在计算效率上，点积注意力远胜于加性注意力。
* **问题：点积的缩放 (The Scaling Problem)** 当Key向量的维度 `dk` 很大时，点积的结果会变得非常大。这会导致 `softmax` 函数的输入值过大，使其进入梯度极小的饱和区（如下图所示）。当梯度接近于零时，反向传播就无法有效地更新模型参数，导致训练困难。

*当输入值很大时，softmax的输出会趋近于0或1，其导数（梯度）会非常小。*

* **解决方案：缩放 (Scaling)** 为了解决这个问题，作者引入了一个关键的改进：**将点积的结果除以 `√dk`**。
  * **原理**：假设Query和Key向量的每个分量都是均值为0、方差为1的独立随机变量，那么它们的点积的方差就等于 `dk`。通过除以 `√dk`，可以将点积结果的方差重新稳定在1左右，从而防止其值过大，避免了softmax的饱和问题。

**最终公式**： 给定查询矩阵 `Q`（维度 `n x dk`）、键矩阵 `K`（维度 `m x dk`）和值矩阵 `V`（维度 `m x dv`），缩放点积注意力的计算公式为：

`Attention(Q, K, V) = softmax(QKᵀ / √dk) * V`

* `QKᵀ`：计算所有Query和Key之间的点积，得到一个 `n x m` 的分数矩阵。
* `/ √dk`：对分数矩阵进行缩放。
* `softmax(...)`：在`K`和`V`的序列长度维度（即`m`）上进行softmax归一化，得到注意力权重矩阵。每一行代表一个Query对所有Key的注意力分布。
* `* V`：用注意力权重矩阵对值矩阵 `V` 进行加权求和，得到最终的输出，其维度为 `n x dv`。

这个公式是Transformer中所有注意力层（自注意力、编码器-解码器注意力）的数学核心

### 多头注意力 (Multi-Head Attention)

这是Transformer模型的另一个创新点，旨在让模型能够从不同的“子空间”或“表示角度”来关注输入信息。

* **为什么需要多头？** 如果只使用一个单一的注意力头，那么所有Query、Key、Value的变换都共享同一套参数。这相当于强迫模型用一种固定的方式去理解所有依赖关系，可能会限制模型的表达能力。例如，一个头可能同时关注了语法结构和语义信息，导致信息混杂。
* **工作原理**： “多头”意味着模型并行地运行**多个**独立的注意力函数（即“头”），然后将它们的结果合并起来。

  * **线性投影**：首先，原始的Query、Key、Value矩阵会通过 `h` 组不同的线性变换（即不同的权重矩阵 `W_Q`, `W_K`, `W_V`）被投影到 `h` 个不同的低维子空间。
    * 每个头的Query、Key维度为 `dk`，Value维度为 `dv`。
    * 通常设置 `dk = dv = d_model / h`，以保证总计算量与单头注意力相当。
  * **并行计算**：在每个投影后的子空间里，独立地应用缩放点积注意力函数。
    * `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
  * **拼接与投影**：将 `h` 个头的输出（每个维度为 `dv`）\*\*拼接（Concatenate）\*\*在一起，形成一个维度为 `h * dv` 的向量。
  * 最后，再通过一个线性变换 `W_O`，将拼接后的向量投影回原始的 `d_model` 维度。
* **最终公式**： `MultiHead(Q, K, V) = Concat(head_1, ..., head_h) * W_O` 其中 `head_i = Attention(Q * W_Q_i, K * W_K_i, V * W_V_i)`
* **优势**：

  * **多样性**：不同的头可以学习到不同的关注模式。例如，有的头可能专注于语法结构（如主谓宾），有的头可能专注于指代关系（如代词和先行词），有的头可能关注语义相似性。
  * **鲁棒性**：即使某个头的注意力失效，其他头仍然可以提供有效信息。
  * **表达能力增强**：相当于模型拥有了多个“专家”，每个专家负责一个特定的方面，共同协作完成任务。

  论文中的实验（Table 3, row A）也证明了，使用多个头（8个）比使用单个头性能更好。

其中：


| 符号         | 含义                         | 如何确定             | 论文中的取值(BASE MODEL) |
| ------------ | ---------------------------- | -------------------- | ------------------------ |
| dmodel       | 模型的主干维度               | 模型的超参数         | 512                      |
| b            | 注意力头的数量               | 模型的超参数         | 8                        |
| dk           | 每个头中 Key 和 Query 的维度 | dk= d_model /h       | 64                       |
| dv           | 每个头中 Value 的维度        | dv=d model /h        | 64                       |
| Q,K,V (单头) | 单个注意力头的矩阵维度       | oatchsze,s或(asize e | (batchsiz,sg leng 1      |
