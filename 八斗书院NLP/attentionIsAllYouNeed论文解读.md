# AttentionIsAllYouNeed论文解读

## 背景

RNN,LSTM,GRU是当时比较序列建模，语言建模和机器翻译中最先进的方法，但是这三种方法都是属于串行结构。串行结构问题在于并行效率低和层数过多是会出现信息丢失的现象。

注意力机制已成为各种任务中引人注目的序列建模和转换模型的组成部分，允许在忽略输入或输出序列中依赖关系距离的情况下进行建模2,19]。然而，在除少数情况27]之外的所有情况下，此类注意力机制都与循环网络结合使用。

在这个工作中，我们提出了Transformer,一种摒弃递归并完全依赖注意力机制来在输入和输出之间建立全局依赖关系的模型架构。Transformer允许实现更高级别的并行化，在八个Pl00 GPU上训练仅十二小时后，即可达到翻译质量的新水平。

## 模型架构

大多数具有竞争力的神经序列转换模型都具有编码器-解码器结构5,2,35]。在这里，编码器将符号表示的输入序列(c1,,xn)映射到连续表示的序列z=(1,,n)。给定五，解码器随后逐个生成符号输出序列(1，，)。在每一步，模型都是自回归的[10]，在生成下一个符号时，将先前生成的符号作为额外的输入。

![2025-09-05_18-46.jpg](https://cdn.jsdelivr.net/gh/zilong-ding/note-gen-image-sync@main/325875f2-d38d-4ecc-805d-9ce497f24662.jpeg)



整个流程可以概括为：

1. **Inputs** (源序列) → 编码器 → 生成**编码器的隐藏表示**。
2. **Outputs** (目标序列，右移) → 解码器 → 生成**解码器的隐藏表示**。
3. 解码器利用其隐藏表示和编码器的隐藏表示，通过注意力机制，预测下一个词。
4. 解码器的最终输出 → **Linear + Softmax** → **Output Probabilities**。
5. 将 `Output Probabilities` 与真实的下一个词进行比较，优化模型。
